"""
å¤å…¸æ–‡é£åˆ†æå™¨

åˆ†æçº¢æ¥¼æ¢¦åŸè‘—çš„è¯­è¨€ç‰¹å¾ï¼Œæå–å¤å…¸æ–‡å­¦çš„é£æ ¼ç‰¹å¾ï¼Œ
ä¸ºåç»­çš„æ–‡é£è½¬æ¢å’Œè¯„ä¼°æä¾›åŸºç¡€æ•°æ®ã€‚

åŠŸèƒ½åŒ…æ‹¬ï¼š
- è¯æ±‡ç»Ÿè®¡åˆ†æï¼šé«˜é¢‘è¯æ±‡ã€æ—¶ä»£ç‰¹å¾è¯ã€æƒ…æ„Ÿè‰²å½©è¯
- å¥å¼ç»“æ„åˆ†æï¼šé•¿çŸ­å¥åˆ†å¸ƒã€å¤å…¸å¥å¼æ¨¡å¼
- ä¿®è¾æ‰‹æ³•åˆ†æï¼šæ¯”å–»ã€å¯¹å¶ã€æ’æ¯”ç­‰æ‰‹æ³•ç»Ÿè®¡
- ç§°è°“ä½“ç³»åˆ†æï¼šç­‰çº§ç§°è°“ã€æ•¬è¯­ä½¿ç”¨
"""

import re
import json
import jieba
import logging
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from collections import Counter, defaultdict

# é…ç½®jiebaä½¿ç”¨çº¢æ¥¼æ¢¦ä¸“ç”¨è¯å…¸
try:
    jieba.set_dictionary("data/processed/hongloumeng_dict_final.txt")
except:
    # å¦‚æœè¯å…¸æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è¯å…¸
    pass

@dataclass
class VocabularyFeatures:
    """è¯æ±‡å±‚é¢ç‰¹å¾"""
    high_freq_classical_words: Dict[str, int]      # é«˜é¢‘å¤å…¸è¯æ±‡
    period_characteristic_words: Dict[str, int]    # æ—¶ä»£ç‰¹å¾è¯æ±‡  
    emotional_color_words: Dict[str, List[str]]    # æƒ…æ„Ÿè‰²å½©è¯æ±‡
    modern_words_detected: List[str]               # æ£€æµ‹åˆ°çš„ç°ä»£è¯æ±‡
    total_word_count: int                          # æ€»è¯æ•°
    classical_word_ratio: float                    # å¤å…¸è¯æ±‡æ¯”ä¾‹

@dataclass
class SentenceFeatures:
    """å¥å¼ç»“æ„ç‰¹å¾"""
    sentence_length_distribution: Dict[str, int]   # å¥é•¿åˆ†å¸ƒï¼ˆçŸ­å¥ã€ä¸­å¥ã€é•¿å¥ï¼‰
    classical_patterns: Dict[str, int]             # å¤å…¸å¥å¼æ¨¡å¼
    punctuation_usage: Dict[str, int]              # æ ‡ç‚¹ç¬¦å·ä½¿ç”¨
    sentence_complexity: float                     # å¥å¼å¤æ‚åº¦
    avg_sentence_length: float                     # å¹³å‡å¥é•¿

@dataclass  
class RhetoricalFeatures:
    """ä¿®è¾æ‰‹æ³•ç‰¹å¾"""
    metaphor_simile_count: int                     # æ¯”å–»è±¡å¾æ•°é‡
    parallelism_count: int                         # å¯¹å¶æ’æ¯”æ•°é‡
    allusion_count: int                            # å…¸æ•…å¼•ç”¨æ•°é‡
    repetition_count: int                          # åå¤é€’è¿›æ•°é‡
    rhetorical_density: float                      # ä¿®è¾å¯†åº¦

@dataclass
class AddressingFeatures:
    """ç§°è°“ä½“ç³»ç‰¹å¾"""
    hierarchical_titles: Dict[str, int]           # ç­‰çº§ç§°è°“ç»Ÿè®¡
    respectful_language: Dict[str, int]           # æ•¬è¯­ä½¿ç”¨ç»Ÿè®¡
    identity_consistency: float                    # èº«ä»½ä¸€è‡´æ€§å¾—åˆ†
    contextual_appropriateness: float              # æƒ…å¢ƒé€‚åº”æ€§å¾—åˆ†

@dataclass
class StyleFeatures:
    """æ•´ä½“æ–‡é£ç‰¹å¾"""
    vocabulary: VocabularyFeatures
    sentence: SentenceFeatures
    rhetorical: RhetoricalFeatures
    addressing: AddressingFeatures
    literary_elegance: float                       # æ–‡å­¦ä¼˜é›…åº¦
    classical_authenticity: float                  # å¤å…¸çœŸå®æ€§

class ClassicalStyleAnalyzer:
    """å¤å…¸æ–‡é£åˆ†æå™¨"""
    
    def __init__(self, hongloumeng_path: str = "data/raw/hongloumeng_80.md"):
        self.hongloumeng_path = Path(hongloumeng_path)
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–è¯æ±‡åº“
        self._init_vocabulary_libraries()
        
        # åŠ è½½çº¢æ¥¼æ¢¦åŸæ–‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.original_text = self._load_original_text()
        
        # åˆ†æç»“æœç¼“å­˜
        self._analysis_cache = {}
        
    def _init_vocabulary_libraries(self):
        """åˆå§‹åŒ–è¯æ±‡åº“"""
        # é«˜é¢‘å¤å…¸è¯æ±‡ï¼ˆä»çº¢æ¥¼æ¢¦ä¸­æå–çš„ç‰¹å¾è¯æ±‡ï¼‰
        self.classical_words = {
            "äººç‰©ç§°è°“": ["å®ç‰", "é»›ç‰", "å®é’—", "å‡¤å§", "è€å¤ªå¤ª", "å§‘å¨˜", "çˆ·", "å¥¶å¥¶"],
            "åœ°ç‚¹åç§°": ["æ€¡çº¢é™¢", "æ½‡æ¹˜é¦†", "ç¨»é¦™æ‘", "è˜…èŠœè‹‘", "å¤§è§‚å›­"],
            "å¤å…¸å½¢å®¹": ["èŠ±å®¹æœˆè²Œ", "å¦‚èŠ±ä¼¼ç‰", "æ²‰é±¼è½é›", "é—­æœˆç¾èŠ±"],
            "æ–‡é›…åŠ¨ä½œ": ["é¢¦è¹™", "å‡çœ¸", "èå°”", "æ€¡ç„¶", "æ‚ ç„¶"],
            "æƒ…æ„Ÿè¡¨è¾¾": ["é¦™æ¶ˆç‰æ®’", "å¿ƒå¦‚åˆ€ç»", "æ³ªå¦‚é›¨ä¸‹", "é»¯ç„¶ç¥ä¼¤"]
        }
        
        # æ—¶ä»£ç‰¹å¾è¯æ±‡
        self.period_words = {
            "ç§°è°“æ•¬è¯­": ["å¥´å©¢", "å©¢å­", "å°çš„", "è§è¿‡", "è¯·å®‰", "æ•¢é—®", "åŠ³çƒ¦"],
            "ç”Ÿæ´»ç”¨å“": ["èƒ­è„‚", "æ°´ç²‰", "ç°ªé’—", "é‡‘é’", "ç‰ä½©", "æ±—å·¾"],
            "å»ºç­‘å™¨ç‰©": ["æ¥¼é˜", "è½©çª—", "é›•æ¢", "ç”»æ ‹", "ç å¸˜", "ç»£å¹•"],
            "æ–‡å­¦å…¸æ•…": ["è¯—ç»", "æ¥šè¾", "å”è¯—", "å®‹è¯", "å¤ç´", "ä¹¦ç”»"]
        }
        
        # ç°ä»£è¯æ±‡ï¼ˆéœ€è¦é¿å…çš„ï¼‰
        self.modern_words = {
            "ç°ä»£æŠ€æœ¯": ["ç”µè¯", "ç”µè„‘", "æ‰‹æœº", "ç½‘ç»œ", "ç”µè§†", "æ±½è½¦"],
            "ç°ä»£è¯æ±‡": ["OK", "æ‹œæ‹œ", "é…·", "æ£’", "è¶…çº§", "éå¸¸"],
            "ç°ä»£è¯­æ³•": ["çš„è¯", "ä»€ä¹ˆçš„", "ä¹‹ç±»çš„", "ç­‰ç­‰"]
        }
        
        # ä¿®è¾æ¨¡å¼
        self.rhetorical_patterns = {
            "æ¯”å–»": [r"å¦‚.*èˆ¬", r"ä¼¼.*æ ·", r"åƒ.*ä¸€æ ·", r".*å¦‚.*"],
            "å¯¹å¶": [r".*å¯¹.*", r".*é….*", r".*ä¸.*ç›¸.*"],
            "æ’æ¯”": [r".*ä¹Ÿ.*ä¹Ÿ.*ä¹Ÿ", r"ä¸€.*ä¸€.*ä¸€.*"],
            "åå¤": [r".*äº†åˆ.*", r".*å†.*å†.*"]
        }
        
        # å¤å…¸å¥å¼æ¨¡å¼
        self.classical_sentence_patterns = {
            "åˆ¤æ–­å¥": [r".*è€…ï¼Œ.*ä¹Ÿ", r".*ï¼Œ.*è€…ä¹Ÿ"],
            "ç–‘é—®å¥": [r".*ä½•.*å“‰", r".*å²‚.*ä¹"],
            "æ„Ÿå¹å¥": [r".*çŸ£", r".*å“‰", r".*ä¹"],
            "çœç•¥å¥": [r".*ä¹‹.*", r".*å…¶.*"]
        }
        
        # ç§°è°“ç­‰çº§ä½“ç³»
        self.hierarchical_titles = {
            "æœ€é«˜çº§": ["è€å¤ªå¤ª", "å¤ªå¤ª", "å¤§äºº"],
            "å°Šæ•¬çº§": ["è€çˆ·", "å¥¶å¥¶", "å§‘å¨˜", "å°‘çˆ·"],
            "å¹³ç­‰çº§": ["å“¥å“¥", "å§å§", "å¼Ÿå¼Ÿ", "å¦¹å¦¹"],
            "è°¦é€Šçº§": ["å¥´å©¢", "å©¢å­", "å°çš„", "åœ¨ä¸‹"]
        }

    def _load_original_text(self) -> str:
        """åŠ è½½çº¢æ¥¼æ¢¦åŸæ–‡"""
        try:
            if self.hongloumeng_path.exists():
                with open(self.hongloumeng_path, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                self.logger.warning(f"çº¢æ¥¼æ¢¦åŸæ–‡æ–‡ä»¶ä¸å­˜åœ¨: {self.hongloumeng_path}")
                return ""
        except Exception as e:
            self.logger.error(f"åŠ è½½çº¢æ¥¼æ¢¦åŸæ–‡å¤±è´¥: {e}")
            return ""

    def analyze_text(self, text: str) -> StyleFeatures:
        """åˆ†ææ–‡æœ¬çš„å¤å…¸æ–‡é£ç‰¹å¾"""
        self.logger.info(f"å¼€å§‹åˆ†ææ–‡æœ¬ï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
        
        # åˆ†è¯é¢„å¤„ç†
        words = list(jieba.cut(text))
        sentences = self._split_sentences(text)
        
        # å„ç»´åº¦åˆ†æ
        vocabulary_features = self._analyze_vocabulary(words)
        sentence_features = self._analyze_sentence_structure(sentences)
        rhetorical_features = self._analyze_rhetorical_devices(text)
        addressing_features = self._analyze_addressing_system(text)
        
        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        literary_elegance = self._calculate_literary_elegance(
            vocabulary_features, sentence_features, rhetorical_features
        )
        classical_authenticity = self._calculate_classical_authenticity(
            vocabulary_features, addressing_features
        )
        
        return StyleFeatures(
            vocabulary=vocabulary_features,
            sentence=sentence_features, 
            rhetorical=rhetorical_features,
            addressing=addressing_features,
            literary_elegance=literary_elegance,
            classical_authenticity=classical_authenticity
        )

    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å¥å¤„ç†"""
        # ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ç¬¦å·åˆ†å¥
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', text)
        return [s.strip() for s in sentences if s.strip()]

    def _analyze_vocabulary(self, words: List[str]) -> VocabularyFeatures:
        """åˆ†æè¯æ±‡ç‰¹å¾"""
        word_counter = Counter(words)
        total_words = len(words)
        
        # é«˜é¢‘å¤å…¸è¯æ±‡ç»Ÿè®¡
        high_freq_classical = {}
        for category, word_list in self.classical_words.items():
            for word in word_list:
                if word in word_counter:
                    high_freq_classical[word] = word_counter[word]
        
        # æ—¶ä»£ç‰¹å¾è¯æ±‡ç»Ÿè®¡  
        period_characteristic = {}
        for category, word_list in self.period_words.items():
            for word in word_list:
                if word in word_counter:
                    period_characteristic[word] = word_counter[word]
        
        # æƒ…æ„Ÿè‰²å½©è¯æ±‡åˆ†ç±»
        emotional_words = {
            "positive": [],
            "negative": [], 
            "neutral": []
        }
        
        positive_patterns = ["èŠ±å®¹", "æœˆè²Œ", "å¦‚èŠ±", "ä¼¼ç‰", "æ€¡ç„¶", "èå°”"]
        negative_patterns = ["é»¯ç„¶", "ç¥ä¼¤", "æ³ªå¦‚", "å¿ƒå¦‚åˆ€", "é¦™æ¶ˆç‰æ®’"]
        
        for word in words:
            if any(pattern in word for pattern in positive_patterns):
                emotional_words["positive"].append(word)
            elif any(pattern in word for pattern in negative_patterns):
                emotional_words["negative"].append(word)
        
        # ç°ä»£è¯æ±‡æ£€æµ‹
        modern_detected = []
        for category, word_list in self.modern_words.items():
            for word in word_list:
                if word in words:
                    modern_detected.append(word)
        
        # è®¡ç®—å¤å…¸è¯æ±‡æ¯”ä¾‹
        classical_count = len(high_freq_classical) + len(period_characteristic)
        classical_ratio = classical_count / total_words if total_words > 0 else 0
        
        return VocabularyFeatures(
            high_freq_classical_words=high_freq_classical,
            period_characteristic_words=period_characteristic,
            emotional_color_words=emotional_words,
            modern_words_detected=modern_detected,
            total_word_count=total_words,
            classical_word_ratio=classical_ratio
        )

    def _analyze_sentence_structure(self, sentences: List[str]) -> SentenceFeatures:
        """åˆ†æå¥å¼ç»“æ„ç‰¹å¾"""
        if not sentences:
            return SentenceFeatures({}, {}, {}, 0.0, 0.0)
            
        # å¥é•¿åˆ†å¸ƒç»Ÿè®¡
        lengths = [len(s) for s in sentences]
        avg_length = sum(lengths) / len(lengths)
        
        length_distribution = {
            "çŸ­å¥(â‰¤10å­—)": sum(1 for l in lengths if l <= 10),
            "ä¸­å¥(11-20å­—)": sum(1 for l in lengths if 11 <= l <= 20),
            "é•¿å¥(>20å­—)": sum(1 for l in lengths if l > 20)
        }
        
        # å¤å…¸å¥å¼æ¨¡å¼è¯†åˆ«
        classical_patterns = {}
        for pattern_name, regex_list in self.classical_sentence_patterns.items():
            count = 0
            for regex in regex_list:
                for sentence in sentences:
                    if re.search(regex, sentence):
                        count += 1
            classical_patterns[pattern_name] = count
        
        # æ ‡ç‚¹ç¬¦å·ä½¿ç”¨ç»Ÿè®¡
        all_text = "".join(sentences)
        punctuation_usage = {
            "å¥å·": all_text.count("ã€‚"),
            "æ„Ÿå¹å·": all_text.count("ï¼"),
            "é—®å·": all_text.count("ï¼Ÿ"),
            "åˆ†å·": all_text.count("ï¼›"),
            "é€—å·": all_text.count("ï¼Œ")
        }
        
        # è®¡ç®—å¥å¼å¤æ‚åº¦
        complexity = self._calculate_sentence_complexity(sentences)
        
        return SentenceFeatures(
            sentence_length_distribution=length_distribution,
            classical_patterns=classical_patterns,
            punctuation_usage=punctuation_usage,
            sentence_complexity=complexity,
            avg_sentence_length=avg_length
        )

    def _analyze_rhetorical_devices(self, text: str) -> RhetoricalFeatures:
        """åˆ†æä¿®è¾æ‰‹æ³•ç‰¹å¾"""
        metaphor_count = 0
        parallelism_count = 0
        allusion_count = 0
        repetition_count = 0
        
        # æ¯”å–»è±¡å¾æ£€æµ‹
        for regex in self.rhetorical_patterns["æ¯”å–»"]:
            metaphor_count += len(re.findall(regex, text))
        
        # å¯¹å¶æ’æ¯”æ£€æµ‹
        for regex in self.rhetorical_patterns["å¯¹å¶"]:
            parallelism_count += len(re.findall(regex, text))
            
        for regex in self.rhetorical_patterns["æ’æ¯”"]:
            parallelism_count += len(re.findall(regex, text))
        
        # å…¸æ•…å¼•ç”¨æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼Œæ£€æµ‹å¸¸è§å…¸æ•…å…³é”®è¯ï¼‰
        allusion_keywords = ["è¯—ç»", "æ¥šè¾", "å”è¯—", "å®‹è¯", "å¤ç´", "ä¹¦ç”»", "å¤ªè™šå¹»å¢ƒ"]
        for keyword in allusion_keywords:
            allusion_count += text.count(keyword)
        
        # åå¤é€’è¿›æ£€æµ‹
        for regex in self.rhetorical_patterns["åå¤"]:
            repetition_count += len(re.findall(regex, text))
        
        # è®¡ç®—ä¿®è¾å¯†åº¦
        total_rhetorical = metaphor_count + parallelism_count + allusion_count + repetition_count
        text_length = len(text)
        rhetorical_density = total_rhetorical / text_length if text_length > 0 else 0
        
        return RhetoricalFeatures(
            metaphor_simile_count=metaphor_count,
            parallelism_count=parallelism_count,
            allusion_count=allusion_count,
            repetition_count=repetition_count,
            rhetorical_density=rhetorical_density
        )

    def _analyze_addressing_system(self, text: str) -> AddressingFeatures:
        """åˆ†æç§°è°“ä½“ç³»ç‰¹å¾"""
        # ç­‰çº§ç§°è°“ç»Ÿè®¡
        hierarchical_counts = {}
        for level, titles in self.hierarchical_titles.items():
            count = 0
            for title in titles:
                count += text.count(title)
            hierarchical_counts[level] = count
        
        # æ•¬è¯­ä½¿ç”¨ç»Ÿè®¡
        respectful_language = {}
        respectful_terms = ["è¯·å®‰", "è§è¿‡", "æ•¢é—®", "åŠ³çƒ¦", "æ­æ•¬", "è°¨éµ"]
        for term in respectful_terms:
            respectful_language[term] = text.count(term)
        
        # èº«ä»½ä¸€è‡´æ€§å¾—åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰
        total_titles = sum(hierarchical_counts.values())
        consistency = 0.8 if total_titles > 0 else 0.0  # ç®€åŒ–è®¡ç®—
        
        # æƒ…å¢ƒé€‚åº”æ€§å¾—åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰
        appropriateness = 0.7 if sum(respectful_language.values()) > 0 else 0.5
        
        return AddressingFeatures(
            hierarchical_titles=hierarchical_counts,
            respectful_language=respectful_language,
            identity_consistency=consistency,
            contextual_appropriateness=appropriateness
        )

    def _calculate_sentence_complexity(self, sentences: List[str]) -> float:
        """è®¡ç®—å¥å¼å¤æ‚åº¦"""
        if not sentences:
            return 0.0
            
        # åŸºäºå¥é•¿å’Œæ ‡ç‚¹ç¬¦å·å¤æ‚åº¦çš„ç®€åŒ–è®¡ç®—
        total_complexity = 0
        for sentence in sentences:
            length_factor = min(len(sentence) / 20, 2.0)  # å¥é•¿å› å­
            punctuation_factor = sentence.count("ï¼Œ") * 0.1 + sentence.count("ï¼›") * 0.2
            total_complexity += length_factor + punctuation_factor
        
        return total_complexity / len(sentences)

    def _calculate_literary_elegance(self, vocab: VocabularyFeatures, 
                                   sentence: SentenceFeatures, 
                                   rhetorical: RhetoricalFeatures) -> float:
        """è®¡ç®—æ–‡å­¦ä¼˜é›…åº¦"""
        # æƒé‡åˆ†é…ï¼šè¯æ±‡40%ï¼Œå¥å¼30%ï¼Œä¿®è¾30%
        vocab_score = vocab.classical_word_ratio
        sentence_score = min(sentence.sentence_complexity / 2.0, 1.0)
        rhetorical_score = min(rhetorical.rhetorical_density * 100, 1.0)
        
        return vocab_score * 0.4 + sentence_score * 0.3 + rhetorical_score * 0.3

    def _calculate_classical_authenticity(self, vocab: VocabularyFeatures,
                                        addressing: AddressingFeatures) -> float:
        """è®¡ç®—å¤å…¸çœŸå®æ€§"""
        # æƒé‡åˆ†é…ï¼šè¯æ±‡60%ï¼Œç§°è°“40%
        vocab_score = vocab.classical_word_ratio
        addressing_score = (addressing.identity_consistency + addressing.contextual_appropriateness) / 2
        
        return vocab_score * 0.6 + addressing_score * 0.4

    def compare_with_original(self, text: str) -> Dict[str, float]:
        """ä¸çº¢æ¥¼æ¢¦åŸè‘—é£æ ¼å¯¹æ¯”"""
        if not self.original_text:
            self.logger.warning("æ— æ³•ä¸åŸè‘—å¯¹æ¯”ï¼šåŸè‘—æ–‡æœ¬æœªåŠ è½½")
            return {}
        
        # åˆ†æç›®æ ‡æ–‡æœ¬
        target_features = self.analyze_text(text)
        
        # åˆ†æåŸè‘—æ–‡æœ¬ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        if "original_features" not in self._analysis_cache:
            self._analysis_cache["original_features"] = self.analyze_text(self.original_text[:50000])  # å–å‰50kå­—ç¬¦
        
        original_features = self._analysis_cache["original_features"]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity_scores = {
            "è¯æ±‡åŒ¹é…åº¦": self._compare_vocabulary(target_features.vocabulary, original_features.vocabulary),
            "å¥å¼ç›¸ä¼¼åº¦": self._compare_sentence_structure(target_features.sentence, original_features.sentence),
            "ä¿®è¾ä¸°å¯Œåº¦": self._compare_rhetorical(target_features.rhetorical, original_features.rhetorical),
            "ç§°è°“å‡†ç¡®åº¦": self._compare_addressing(target_features.addressing, original_features.addressing),
            "æ•´ä½“ç›¸ä¼¼åº¦": (target_features.literary_elegance + target_features.classical_authenticity) / 2
        }
        
        return similarity_scores

    def _compare_vocabulary(self, target: VocabularyFeatures, original: VocabularyFeatures) -> float:
        """æ¯”è¾ƒè¯æ±‡ç‰¹å¾"""
        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
        ratio_diff = abs(target.classical_word_ratio - original.classical_word_ratio)
        return max(0, 1 - ratio_diff)

    def _compare_sentence_structure(self, target: SentenceFeatures, original: SentenceFeatures) -> float:
        """æ¯”è¾ƒå¥å¼ç»“æ„"""
        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
        length_diff = abs(target.avg_sentence_length - original.avg_sentence_length) / original.avg_sentence_length
        complexity_diff = abs(target.sentence_complexity - original.sentence_complexity) / original.sentence_complexity
        return max(0, 1 - (length_diff + complexity_diff) / 2)

    def _compare_rhetorical(self, target: RhetoricalFeatures, original: RhetoricalFeatures) -> float:
        """æ¯”è¾ƒä¿®è¾ç‰¹å¾"""
        density_diff = abs(target.rhetorical_density - original.rhetorical_density) / max(original.rhetorical_density, 0.01)
        return max(0, 1 - density_diff)

    def _compare_addressing(self, target: AddressingFeatures, original: AddressingFeatures) -> float:
        """æ¯”è¾ƒç§°è°“ç‰¹å¾"""
        consistency_diff = abs(target.identity_consistency - original.identity_consistency)
        appropriateness_diff = abs(target.contextual_appropriateness - original.contextual_appropriateness)
        return max(0, 1 - (consistency_diff + appropriateness_diff) / 2)

    def save_analysis_result(self, features: StyleFeatures, output_path: str):
        """ä¿å­˜åˆ†æç»“æœ"""
        result = asdict(features)
        result["analysis_timestamp"] = "2025-01-24"
        result["analyzer_version"] = "1.0.0"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")

    def generate_analysis_report(self, features: StyleFeatures) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = f"""
# å¤å…¸æ–‡é£åˆ†ææŠ¥å‘Š

## ğŸ“Š è¯æ±‡ç‰¹å¾
- æ€»è¯æ•°: {features.vocabulary.total_word_count}
- å¤å…¸è¯æ±‡æ¯”ä¾‹: {features.vocabulary.classical_word_ratio:.2%}
- æ£€æµ‹åˆ°çš„ç°ä»£è¯æ±‡: {len(features.vocabulary.modern_words_detected)} ä¸ª

## ğŸ“ å¥å¼ç‰¹å¾  
- å¹³å‡å¥é•¿: {features.sentence.avg_sentence_length:.1f} å­—
- å¥å¼å¤æ‚åº¦: {features.sentence.sentence_complexity:.2f}
- å¤å…¸å¥å¼ä½¿ç”¨: {sum(features.sentence.classical_patterns.values())} å¤„

## ğŸ­ ä¿®è¾ç‰¹å¾
- æ¯”å–»è±¡å¾: {features.rhetorical.metaphor_simile_count} å¤„
- å¯¹å¶æ’æ¯”: {features.rhetorical.parallelism_count} å¤„  
- å…¸æ•…å¼•ç”¨: {features.rhetorical.allusion_count} å¤„
- ä¿®è¾å¯†åº¦: {features.rhetorical.rhetorical_density:.4f}

## ğŸ‘¤ ç§°è°“ç‰¹å¾
- èº«ä»½ä¸€è‡´æ€§: {features.addressing.identity_consistency:.2%}
- æƒ…å¢ƒé€‚åº”æ€§: {features.addressing.contextual_appropriateness:.2%}

## ğŸ¯ ç»¼åˆè¯„åˆ†
- æ–‡å­¦ä¼˜é›…åº¦: {features.literary_elegance:.2%}
- å¤å…¸çœŸå®æ€§: {features.classical_authenticity:.2%}

## ğŸ’¡ æ”¹è¿›å»ºè®®
"""
        
        # æ ¹æ®åˆ†æç»“æœç»™å‡ºå»ºè®®
        if features.vocabulary.classical_word_ratio < 0.3:
            report += "- å»ºè®®å¢åŠ å¤å…¸è¯æ±‡çš„ä½¿ç”¨ï¼Œå‡å°‘ç°ä»£åŒ–è¡¨è¾¾\n"
        
        if features.rhetorical.rhetorical_density < 0.01:
            report += "- å»ºè®®å¢åŠ ä¿®è¾æ‰‹æ³•ï¼Œå¦‚æ¯”å–»ã€å¯¹å¶ç­‰ï¼Œæå‡æ–‡å­¦æ€§\n"
            
        if features.sentence.avg_sentence_length < 10:
            report += "- å»ºè®®é€‚å½“å¢åŠ å¥å­é•¿åº¦ï¼Œä½¿ç”¨æ›´å¤šçš„å¤åˆå¥å¼\n"
            
        if len(features.vocabulary.modern_words_detected) > 0:
            report += f"- å‘ç°ç°ä»£è¯æ±‡: {', '.join(features.vocabulary.modern_words_detected[:5])}ï¼Œå»ºè®®æ›¿æ¢ä¸ºå¤å…¸è¡¨è¾¾\n"
        
        return report 