"""
æ™ºèƒ½æ–‡é£è½¬æ¢å™¨

å°†ç°ä»£åŒ–çš„AIç”Ÿæˆæ–‡æœ¬è½¬æ¢ä¸ºç¬¦åˆçº¢æ¥¼æ¢¦åŸè‘—é£æ ¼çš„å¤å…¸æ–‡å­¦æ–‡æœ¬ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- è¯æ±‡å±‚é¢è½¬æ¢ï¼šç°ä»£è¯æ±‡â†’å¤å…¸è¯æ±‡æ˜ å°„
- å¥å¼å±‚é¢é‡æ„ï¼šè¯­åºè°ƒæ•´å’ŒåŠ©è¯æ·»åŠ 
- ä¿®è¾å±‚é¢å¢å¼ºï¼šæ¯”å–»ã€å¯¹å¶ç­‰ä¿®è¾æ‰‹æ³•
- è¯­å¢ƒå±‚é¢ä¼˜åŒ–ï¼šäººç‰©èº«ä»½å’Œæƒ…å¢ƒé€‚é…
"""

import re
import json
import jieba
import logging
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from collections import defaultdict

from .classical_style_analyzer import ClassicalStyleAnalyzer, StyleFeatures
from .style_template_library import StyleTemplateLibrary, DialogueType, NarrativeType

@dataclass
class ConversionResult:
    """è½¬æ¢ç»“æœ"""
    original_text: str                      # åŸå§‹æ–‡æœ¬
    converted_text: str                     # è½¬æ¢åæ–‡æœ¬
    conversion_operations: List[str]        # è½¬æ¢æ“ä½œè®°å½•
    vocabulary_changes: Dict[str, str]      # è¯æ±‡æ›¿æ¢è®°å½•
    sentence_adjustments: List[str]         # å¥å¼è°ƒæ•´è®°å½•
    rhetorical_enhancements: List[str]      # ä¿®è¾å¢å¼ºè®°å½•
    quality_score: float                    # è½¬æ¢è´¨é‡è¯„åˆ†
    confidence_score: float                 # è½¬æ¢ç½®ä¿¡åº¦

@dataclass
class ConversionConfig:
    """è½¬æ¢é…ç½®"""
    vocabulary_level: str = "high"          # è¯æ±‡è½¬æ¢å¼ºåº¦ (low/medium/high)
    sentence_restructure: bool = True       # æ˜¯å¦é‡æ„å¥å¼
    add_rhetorical_devices: bool = True     # æ˜¯å¦æ·»åŠ ä¿®è¾
    preserve_meaning: bool = True           # æ˜¯å¦ä¿æŒè¯­ä¹‰
    character_context: Optional[str] = None # äººç‰©èº«ä»½ä¸Šä¸‹æ–‡
    scene_context: Optional[str] = None     # åœºæ™¯ä¸Šä¸‹æ–‡

class IntelligentStyleConverter:
    """æ™ºèƒ½æ–‡é£è½¬æ¢å™¨"""
    
    def __init__(self, 
                 analyzer: Optional[ClassicalStyleAnalyzer] = None,
                 template_library: Optional[StyleTemplateLibrary] = None):
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–åˆ†æå™¨å’Œæ¨¡æ¿åº“
        self.analyzer = analyzer or ClassicalStyleAnalyzer()
        self.template_library = template_library or StyleTemplateLibrary()
        
        # åˆå§‹åŒ–è½¬æ¢è§„åˆ™
        self._init_conversion_rules()
        
        # è½¬æ¢å†å²
        self.conversion_history: List[ConversionResult] = []
        
    def _init_conversion_rules(self):
        """åˆå§‹åŒ–è½¬æ¢è§„åˆ™"""
        
        # è¯æ±‡æ˜ å°„è§„åˆ™
        self.vocabulary_mapping = {
            # å¸¸è§ç°ä»£è¯æ±‡åˆ°å¤å…¸è¯æ±‡çš„æ˜ å°„
            "å¾ˆ": "ç”š", "éå¸¸": "æ", "ç‰¹åˆ«": "åˆ†å¤–",
            "è¯´": "é“", "è¯´è¯": "è¨€è¯­", "è¯´é“": "è¯´é“",
            "çœ‹": "ç§", "çœ‹è§": "åªè§", "çœ‹åˆ°": "ç§è§",
            "ç€æ€¥": "å¿ƒç„¦", "æ‹…å¿ƒ": "æ‹…å¿§", "å®³æ€•": "æƒŠæ",
            "é«˜å…´": "æ¬¢å–œ", "å¼€å¿ƒ": "å–œæ‚¦", "ç”Ÿæ°”": "æ¼æ€’",
            "æ¼‚äº®": "æ ‡è‡´", "ç¾ä¸½": "å¨‡ç¾", "ä¸‘é™‹": "ä¸‘æ¶",
            "èªæ˜": "ä¼¶ä¿", "ç¬¨": "æ„šé’", "å¯çˆ±": "å¯äºº",
            "æˆ¿é—´": "å±‹å­", "å®¶": "åºœç¬¬", "åƒ": "ç”¨",
            "èµ°": "å»", "æ¥": "æ¥äº†", "åœ¨": "æ­£åœ¨",
            "çš„": "çš„", "äº†": "äº†", "ç€": "ç€",
            
            # æƒ…æ„Ÿè¡¨è¾¾è¯æ±‡
            "å“­": "å“­æ³£", "ç¬‘": "å«ç¬‘", "æƒ³": "æ€é‡",
            "ç­‰": "ç­‰å€™", "æ‰¾": "å¯»", "ç»™": "ä¸",
            "æ‹¿": "å–", "æ”¾": "æ", "å": "åä¸‹",
            
            # ç–¾ç—…ç›¸å…³
            "ç”Ÿç—…": "èº«å­ä¸å¥½", "ç—…äº†": "èº«å­æŠ±æ™", 
            "æ„Ÿå†’": "ä¼¤é£", "å‘çƒ§": "å‘çƒ­",
            
            # åŠ¨ä½œè¯æ±‡
            "èµ°è·¯": "è¡Œèµ°", "è·‘": "å¥”è·‘", "è·³": "è¹¦è·³",
            "ç«™": "ç«‹", "èºº": "å§", "ç¡": "å®‰å¯",
        }
        
        # ç§°è°“æ˜ å°„è§„åˆ™
        self.addressing_mapping = {
            "ä»–": "ä»–", "å¥¹": "å¥¹", "ä½ ": "ä½ ",
            "æˆ‘": "æˆ‘", "æˆ‘ä»¬": "å’±ä»¬", "ä½ ä»¬": "ä½ ä»¬",
            "è€çˆ·": "è€çˆ·", "å¤ªå¤ª": "å¤ªå¤ª", "å¥¶å¥¶": "å¥¶å¥¶",
            "å°å§": "å§‘å¨˜", "å¤«äºº": "å¤«äºº", "å…¬å­": "å…¬å­",
        }
        
        # å¥å¼æ¨¡å¼
        self.sentence_patterns = {
            "åªè§": ["åªè§{subject}{action}", "åªè§{subject}æ­£{action}"],
            "å´è¯´": ["å´è¯´{subject}{action}", "å´è¯´{subject}åœ¨{place}{action}"],
            "ä½†è§": ["ä½†è§{description}", "ä½†è§{subject}{description}"],
            "åŸæ¥": ["åŸæ¥{explanation}", "åŸæ¥{subject}{reason}"],
        }
        
        # ä¿®è¾æ¨¡å¼
        self.rhetorical_patterns = {
            "æ¯”å–»": ["{subject}å¦‚{object}èˆ¬{attribute}", "{subject}ä¼¼{object}æ ·{attribute}"],
            "å¯¹å¶": ["{phrase1}ï¼Œ{phrase2}", "{action1}å¯¹{action2}"],
            "æ’æ¯”": ["{phrase1}ï¼Œ{phrase2}ï¼Œ{phrase3}"],
        }
        
        # åŠ©è¯æ·»åŠ è§„åˆ™
        self.auxiliary_words = ["ä¹‹", "ä¹Ÿ", "è€…", "çŸ£", "ä¹", "ç„‰", "å“‰"]
        
    def convert_text(self, 
                    text: str, 
                    config: Optional[ConversionConfig] = None) -> ConversionResult:
        """è½¬æ¢æ–‡æœ¬é£æ ¼"""
        
        if config is None:
            config = ConversionConfig()
            
        self.logger.info(f"å¼€å§‹è½¬æ¢æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)}")
        
        # è®°å½•è½¬æ¢æ“ä½œ
        operations = []
        vocabulary_changes = {}
        sentence_adjustments = []
        rhetorical_enhancements = []
        
        # åˆ†å¥å¤„ç†
        sentences = self._split_sentences(text)
        converted_sentences = []
        
        for sentence in sentences:
            if not sentence.strip():
                converted_sentences.append(sentence)
                continue
                
            # 1. è¯æ±‡å±‚é¢è½¬æ¢
            sentence_after_vocab = self._convert_vocabulary(sentence, config)
            vocab_changes = self._get_vocabulary_changes(sentence, sentence_after_vocab)
            vocabulary_changes.update(vocab_changes)
            
            # 2. å¥å¼å±‚é¢é‡æ„
            sentence_after_structure = sentence_after_vocab
            if config.sentence_restructure:
                sentence_after_structure = self._restructure_sentence(sentence_after_vocab, config)
                if sentence_after_structure != sentence_after_vocab:
                    sentence_adjustments.append(f"å¥å¼è°ƒæ•´: {sentence_after_vocab} â†’ {sentence_after_structure}")
            
            # 3. ä¿®è¾å±‚é¢å¢å¼º
            sentence_after_rhetoric = sentence_after_structure
            if config.add_rhetorical_devices:
                sentence_after_rhetoric = self._enhance_rhetoric(sentence_after_structure, config)
                if sentence_after_rhetoric != sentence_after_structure:
                    rhetorical_enhancements.append(f"ä¿®è¾å¢å¼º: {sentence_after_structure} â†’ {sentence_after_rhetoric}")
            
            # 4. è¯­å¢ƒå±‚é¢ä¼˜åŒ–
            final_sentence = self._optimize_context(sentence_after_rhetoric, config)
            
            converted_sentences.append(final_sentence)
            operations.append(f"å¥å­è½¬æ¢: {sentence.strip()} â†’ {final_sentence.strip()}")
        
        # ç»„åˆç»“æœ
        converted_text = "".join(converted_sentences)
        
        # è®¡ç®—è´¨é‡è¯„åˆ†
        quality_score = self._calculate_quality_score(text, converted_text)
        confidence_score = self._calculate_confidence_score(len(vocabulary_changes), len(operations))
        
        # åˆ›å»ºè½¬æ¢ç»“æœ
        result = ConversionResult(
            original_text=text,
            converted_text=converted_text,
            conversion_operations=operations,
            vocabulary_changes=vocabulary_changes,
            sentence_adjustments=sentence_adjustments,
            rhetorical_enhancements=rhetorical_enhancements,
            quality_score=quality_score,
            confidence_score=confidence_score
        )
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        self.conversion_history.append(result)
        
        self.logger.info(f"è½¬æ¢å®Œæˆï¼Œè´¨é‡è¯„åˆ†: {quality_score:.2f}, ç½®ä¿¡åº¦: {confidence_score:.2f}")
        
        return result
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å¥"""
        # æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å¥ï¼Œä¿ç•™æ ‡ç‚¹
        pattern = r'([ã€‚ï¼ï¼Ÿï¼›â€¦]+|[\n\r]+)'
        parts = re.split(pattern, text)
        
        sentences = []
        current_sentence = ""
        
        for i, part in enumerate(parts):
            if re.match(r'[ã€‚ï¼ï¼Ÿï¼›â€¦\n\r]+', part):
                current_sentence += part
                sentences.append(current_sentence)
                current_sentence = ""
            else:
                current_sentence += part
        
        if current_sentence:
            sentences.append(current_sentence)
            
        return sentences
    
    def _convert_vocabulary(self, sentence: str, config: ConversionConfig) -> str:
        """è¯æ±‡å±‚é¢è½¬æ¢"""
        
        # åˆ†è¯
        words = list(jieba.cut(sentence))
        converted_words = []
        
        for word in words:
            # æŸ¥æ‰¾æ˜ å°„
            if word in self.vocabulary_mapping:
                converted_word = self.vocabulary_mapping[word]
                converted_words.append(converted_word)
            else:
                # æ£€æŸ¥æ˜¯å¦ä¸ºç°ä»£åŒ–è¯æ±‡ï¼Œéœ€è¦è½¬æ¢
                converted_word = self._find_classical_equivalent(word, config)
                converted_words.append(converted_word)
        
        return "".join(converted_words)
    
    def _find_classical_equivalent(self, word: str, config: ConversionConfig) -> str:
        """å¯»æ‰¾å¤å…¸å¯¹ç­‰è¯æ±‡"""
        
        # åŸºäºè¯­å¢ƒå’Œå­—å…¸æŸ¥æ‰¾
        if len(word) == 1:
            return word  # å•å­—ä¸€èˆ¬ä¿æŒä¸å˜
        
        # æƒ…æ„Ÿè¯æ±‡è½¬æ¢
        emotion_mapping = {
            "æ¿€åŠ¨": "å…´å¥‹", "æ²®ä¸§": "æ²®ä¸§", "æ„¤æ€’": "æ„¤æ…¨",
            "å¿«ä¹": "å¿«æ´»", "æ‚²ä¼¤": "æ‚²æˆš", "æƒŠè®¶": "æƒŠå¥‡",
        }
        
        if word in emotion_mapping:
            return emotion_mapping[word]
        
        # åŠ¨ä½œè¯æ±‡è½¬æ¢
        action_mapping = {
            "å…³å¿ƒ": "å…³æ€€", "å¸®åŠ©": "å¸®è¡¬", "ä¿æŠ¤": "æŠ¤ä½‘",
            "å†³å®š": "å†³æ„", "é€‰æ‹©": "æ‹£é€‰", "è€ƒè™‘": "æ€å¿–",
        }
        
        if word in action_mapping:
            return action_mapping[word]
        
        return word  # é»˜è®¤ä¿æŒåŸè¯
    
    def _restructure_sentence(self, sentence: str, config: ConversionConfig) -> str:
        """å¥å¼å±‚é¢é‡æ„"""
        
        # åŸºæœ¬çš„å¥å¼è°ƒæ•´
        restructured = sentence
        
        # 1. æ·»åŠ å¤å…¸å¼€å¤´è¯
        if self._should_add_classical_start(sentence):
            restructured = self._add_classical_start(restructured)
        
        # 2. è°ƒæ•´è¯­åº
        restructured = self._adjust_word_order(restructured)
        
        # 3. æ·»åŠ åŠ©è¯
        restructured = self._add_auxiliary_words(restructured, config)
        
        return restructured
    
    def _should_add_classical_start(self, sentence: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ·»åŠ å¤å…¸å¼€å¤´"""
        # å¦‚æœå¥å­è¾ƒé•¿ä¸”æ²¡æœ‰å¤å…¸å¼€å¤´è¯ï¼Œåˆ™æ·»åŠ 
        classical_starts = ["åªè§", "å´è¯´", "ä½†è§", "åŸæ¥", "å¿½ç„¶", "å¿½è§"]
        
        for start in classical_starts:
            if sentence.startswith(start):
                return False
        
        # åˆ¤æ–­å¥å­ç±»å‹
        if len(sentence) > 10 and not sentence.startswith(("ä»–", "å¥¹", "æˆ‘", "ä½ ")):
            return True
            
        return False
    
    def _add_classical_start(self, sentence: str) -> str:
        """æ·»åŠ å¤å…¸å¼€å¤´è¯"""
        
        # æ ¹æ®å¥å­å†…å®¹é€‰æ‹©åˆé€‚çš„å¼€å¤´
        if "çœ‹" in sentence or "ç§" in sentence:
            return "åªè§" + sentence
        elif "è¯´" in sentence or "é“" in sentence:
            return "å´è¯´" + sentence
        elif "æ˜¯" in sentence or "æœ‰" in sentence:
            return "åŸæ¥" + sentence
        else:
            return "ä½†è§" + sentence
    
    def _adjust_word_order(self, sentence: str) -> str:
        """è°ƒæ•´è¯­åº"""
        
        # ç®€å•çš„è¯­åºè°ƒæ•´è§„åˆ™
        adjusted = sentence
        
        # å¤„ç† "å¾ˆ+å½¢å®¹è¯" ç»“æ„
        pattern = r'å¾ˆ(\w+)'
        adjusted = re.sub(pattern, r'ç”šæ˜¯\1', adjusted)
        
        # å¤„ç† "éå¸¸+å½¢å®¹è¯" ç»“æ„  
        pattern = r'éå¸¸(\w+)'
        adjusted = re.sub(pattern, r'ææ˜¯\1', adjusted)
        
        return adjusted
    
    def _add_auxiliary_words(self, sentence: str, config: ConversionConfig) -> str:
        """æ·»åŠ åŠ©è¯"""
        
        if config.vocabulary_level != "high":
            return sentence
        
        # åœ¨é€‚å½“ä½ç½®æ·»åŠ åŠ©è¯
        enhanced = sentence
        
        # åœ¨å¥æœ«æ·»åŠ è¯­æ°”è¯ï¼ˆæ¦‚ç‡æ€§ï¼‰
        if not re.search(r'[ã€‚ï¼ï¼Ÿ]$', enhanced):
            return enhanced
        
        # æ ¹æ®å¥å­è¯­æ°”é€‰æ‹©åŠ©è¯
        if "ï¼Ÿ" in enhanced:
            # ç–‘é—®å¥ï¼Œå¯èƒ½æ·»åŠ "ä¹"
            enhanced = enhanced.replace("ï¼Ÿ", "ä¹ï¼Ÿ")
        elif "ï¼" in enhanced:
            # æ„Ÿå¹å¥ï¼Œå¯èƒ½æ·»åŠ "å“‰"
            enhanced = enhanced.replace("ï¼", "å“‰ï¼")
        
        return enhanced
    
    def _enhance_rhetoric(self, sentence: str, config: ConversionConfig) -> str:
        """ä¿®è¾å±‚é¢å¢å¼º"""
        
        enhanced = sentence
        
        # 1. å¯»æ‰¾å¯ä»¥æ·»åŠ æ¯”å–»çš„åœ°æ–¹
        enhanced = self._add_metaphor(enhanced)
        
        # 2. å¯»æ‰¾å¯ä»¥å½¢æˆå¯¹å¶çš„ç»“æ„
        enhanced = self._add_parallelism(enhanced)
        
        return enhanced
    
    def _add_metaphor(self, sentence: str) -> str:
        """æ·»åŠ æ¯”å–»ä¿®è¾"""
        
        # ç®€å•çš„æ¯”å–»å¢å¼º
        metaphor_patterns = {
            r'(\w+)å¾ˆç¾': r'\1å¦‚èŠ±ä¼¼ç‰',
            r'(\w+)å¾ˆèªæ˜': r'\1ä¼¶ä¿å¦‚å†°é›ª', 
            r'(\w+)å¾ˆç€æ€¥': r'\1å¿ƒå¦‚ç«ç„š',
            r'(\w+)å¾ˆä¼¤å¿ƒ': r'\1å¿ƒå¦‚åˆ€ç»',
        }
        
        enhanced = sentence
        for pattern, replacement in metaphor_patterns.items():
            enhanced = re.sub(pattern, replacement, enhanced)
            
        return enhanced
    
    def _add_parallelism(self, sentence: str) -> str:
        """æ·»åŠ å¯¹å¶ä¿®è¾"""
        
        # å¯»æ‰¾å¯ä»¥å½¢æˆå¯¹å¶çš„ç»“æ„ï¼ˆè¿™é‡Œæ˜¯ç®€åŒ–å®ç°ï¼‰
        enhanced = sentence
        
        # ä¾‹å¦‚ï¼šå°†"é«˜å…´å’Œå¿«ä¹"è½¬æ¢ä¸º"æ¬¢å–œå¯¹å¿«æ´»"
        pattern = r'(\w+)å’Œ(\w+)'
        if re.search(pattern, enhanced):
            enhanced = re.sub(pattern, r'\1å¯¹\2', enhanced)
        
        return enhanced
    
    def _optimize_context(self, sentence: str, config: ConversionConfig) -> str:
        """è¯­å¢ƒå±‚é¢ä¼˜åŒ–"""
        
        optimized = sentence
        
        # æ ¹æ®äººç‰©èº«ä»½è°ƒæ•´è¯­è¨€
        if config.character_context:
            optimized = self._adjust_for_character(optimized, config.character_context)
        
        # æ ¹æ®åœºæ™¯è°ƒæ•´è¯­è¨€
        if config.scene_context:
            optimized = self._adjust_for_scene(optimized, config.scene_context)
        
        return optimized
    
    def _adjust_for_character(self, sentence: str, character: str) -> str:
        """æ ¹æ®äººç‰©èº«ä»½è°ƒæ•´è¯­è¨€"""
        
        # ä¸åŒäººç‰©çš„è¯­è¨€ç‰¹ç‚¹
        character_styles = {
            "è´¾å®ç‰": {"ç‰¹ç‚¹": "æ¸©æ–‡å°”é›…", "å¸¸ç”¨è¯": ["é¢¦å„¿", "å¦¹å¦¹", "å§å§"]},
            "æ—é»›ç‰": {"ç‰¹ç‚¹": "æ–‡é›…å§”å©‰", "å¸¸ç”¨è¯": ["å®å“¥å“¥", "å§å¦¹ä»¬"]},
            "ç‹ç†™å‡¤": {"ç‰¹ç‚¹": "åˆ©è½ç›´æ¥", "å¸¸ç”¨è¯": ["ä½ ä»¬", "å’±ä»¬", "æˆ‘è¯´"]},
            "è´¾æ¯": {"ç‰¹ç‚¹": "å¨ä¸¥æ…ˆç¥¥", "å¸¸ç”¨è¯": ["å¥½å­©å­", "æˆ‘çš„å¿ƒè‚"]},
        }
        
        if character in character_styles:
            style = character_styles[character]
            # è¿™é‡Œå¯ä»¥æ ¹æ®äººç‰©ç‰¹ç‚¹è¿›è¡Œæ›´ç»†è‡´çš„è°ƒæ•´
            pass
        
        return sentence
    
    def _adjust_for_scene(self, sentence: str, scene: str) -> str:
        """æ ¹æ®åœºæ™¯è°ƒæ•´è¯­è¨€"""
        
        scene_styles = {
            "æ­£å¼åœºåˆ": {"ç‰¹ç‚¹": "åº„é‡ä¸¥è‚ƒ", "é¿å…": ["ä¿—è¯­", "ç©ç¬‘"]},
            "ç§äººå¯¹è¯": {"ç‰¹ç‚¹": "äº²å¯†è‡ªç„¶", "å…è®¸": ["æ˜µç§°", "ä¿—è¯­"]},
            "è¯—è¯åœºåˆ": {"ç‰¹ç‚¹": "æ–‡é›…é«˜æ·±", "åå¥½": ["å…¸æ•…", "å¯¹å¶"]},
        }
        
        if scene in scene_styles:
            style = scene_styles[scene]
            # æ ¹æ®åœºæ™¯ç‰¹ç‚¹è°ƒæ•´
            pass
        
        return sentence
    
    def _get_vocabulary_changes(self, original: str, converted: str) -> Dict[str, str]:
        """è·å–è¯æ±‡å˜åŒ–è®°å½•"""
        
        changes = {}
        
        # ç®€å•çš„è¯æ±‡å¯¹æ¯”ï¼ˆå®é™…åº”è¯¥æ›´å¤æ‚ï¼‰
        orig_words = set(jieba.cut(original))
        conv_words = set(jieba.cut(converted))
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦æ›´ç²¾ç¡®çš„å¯¹åº”å…³ç³»
        for orig_word in orig_words:
            if orig_word in self.vocabulary_mapping:
                mapped_word = self.vocabulary_mapping[orig_word]
                if mapped_word in conv_words:
                    changes[orig_word] = mapped_word
        
        return changes
    
    def _calculate_quality_score(self, original: str, converted: str) -> float:
        """è®¡ç®—è½¬æ¢è´¨é‡è¯„åˆ†"""
        
        # åŸºäºå¤šä¸ªç»´åº¦è®¡ç®—è´¨é‡è¯„åˆ†
        
        # 1. é•¿åº¦å˜åŒ–åˆç†æ€§ (æƒé‡: 0.2)
        length_ratio = len(converted) / len(original) if len(original) > 0 else 1.0
        length_score = 1.0 - abs(length_ratio - 1.0) * 0.5  # ç†æƒ³æ¯”ä¾‹1.0-1.3
        length_score = max(0.0, min(1.0, length_score))
        
        # 2. å¤å…¸è¯æ±‡æ¯”ä¾‹ (æƒé‡: 0.3)
        classical_score = self._calculate_classical_ratio(converted)
        
        # 3. å¥å¼å¤æ‚åº¦ (æƒé‡: 0.2)
        complexity_score = self._calculate_complexity_score(converted)
        
        # 4. è¯­ä¹‰ä¿æŒåº¦ (æƒé‡: 0.3)
        semantic_score = self._calculate_semantic_preservation(original, converted)
        
        # åŠ æƒå¹³å‡
        total_score = (length_score * 0.2 + 
                      classical_score * 0.3 + 
                      complexity_score * 0.2 + 
                      semantic_score * 0.3)
        
        return round(total_score, 3)
    
    def _calculate_classical_ratio(self, text: str) -> float:
        """è®¡ç®—å¤å…¸è¯æ±‡æ¯”ä¾‹"""
        
        words = list(jieba.cut(text))
        if not words:
            return 0.0
        
        classical_count = 0
        for word in words:
            if len(word) > 1 and self._is_classical_word(word):
                classical_count += 1
        
        return classical_count / len(words)
    
    def _is_classical_word(self, word: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¤å…¸è¯æ±‡"""
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤å…¸è¯æ±‡åº“ä¸­
        classical_indicators = [
            "åªè§", "å´è¯´", "ä½†è§", "åŸæ¥", 
            "ç”šæ˜¯", "ææ˜¯", "é¢¦å„¿", "æ€¡çº¢é™¢",
            "ç§è§", "æ€é‡", "ä¼¶ä¿", "æ ‡è‡´"
        ]
        
        return word in classical_indicators or word in self.vocabulary_mapping.values()
    
    def _calculate_complexity_score(self, text: str) -> float:
        """è®¡ç®—å¥å¼å¤æ‚åº¦è¯„åˆ†"""
        
        # åŸºäºå¥é•¿åˆ†å¸ƒå’Œç»“æ„å¤æ‚åº¦
        sentences = self._split_sentences(text)
        if not sentences:
            return 0.0
        
        total_score = 0.0
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # å¥é•¿è¯„åˆ†ï¼ˆ10-30å­—ä¸ºæœ€ä½³ï¼‰
            length = len(sentence)
            if 10 <= length <= 30:
                length_score = 1.0
            elif length < 10:
                length_score = length / 10.0
            else:
                length_score = 30.0 / length
            
            # ç»“æ„å¤æ‚åº¦ï¼ˆåŸºäºæ ‡ç‚¹å’Œè¿è¯ï¼‰
            complexity_indicators = sentence.count('ï¼Œ') + sentence.count('ï¼›') + sentence.count('ï¼š')
            complexity_score = min(1.0, complexity_indicators / 3.0)
            
            sentence_score = (length_score + complexity_score) / 2.0
            total_score += sentence_score
        
        return total_score / len(sentences)
    
    def _calculate_semantic_preservation(self, original: str, converted: str) -> float:
        """è®¡ç®—è¯­ä¹‰ä¿æŒåº¦"""
        
        # ç®€åŒ–å®ç°ï¼šåŸºäºå…³é”®è¯ä¿æŒ
        orig_words = set(jieba.cut(original))
        conv_words = set(jieba.cut(converted))
        
        # ç§»é™¤æ ‡ç‚¹å’Œåœç”¨è¯
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ'}
        orig_words = {w for w in orig_words if w not in stop_words and len(w) > 1}
        conv_words = {w for w in conv_words if w not in stop_words and len(w) > 1}
        
        if not orig_words:
            return 1.0
        
        # è®¡ç®—ä¿æŒçš„å…³é”®æ¦‚å¿µæ¯”ä¾‹
        preserved_concepts = 0
        for orig_word in orig_words:
            # ç›´æ¥ä¿æŒ
            if orig_word in conv_words:
                preserved_concepts += 1
            # é€šè¿‡æ˜ å°„ä¿æŒ
            elif orig_word in self.vocabulary_mapping:
                mapped_word = self.vocabulary_mapping[orig_word]
                if mapped_word in conv_words:
                    preserved_concepts += 1
        
        return preserved_concepts / len(orig_words)
    
    def _calculate_confidence_score(self, vocab_changes: int, total_operations: int) -> float:
        """è®¡ç®—è½¬æ¢ç½®ä¿¡åº¦"""
        
        # åŸºäºè½¬æ¢æ“ä½œçš„æ•°é‡å’Œå¤æ‚åº¦
        if total_operations == 0:
            return 1.0
        
        # è¯æ±‡æ›¿æ¢çš„å¯é æ€§è¾ƒé«˜
        vocab_confidence = min(1.0, vocab_changes / (total_operations * 0.7))
        
        # æ•´ä½“æ“ä½œçš„åˆç†æ€§
        operation_confidence = min(1.0, total_operations / 10.0)
        
        return (vocab_confidence + operation_confidence) / 2.0
    
    def batch_convert(self, texts: List[str], config: Optional[ConversionConfig] = None) -> List[ConversionResult]:
        """æ‰¹é‡è½¬æ¢æ–‡æœ¬"""
        
        results = []
        for text in texts:
            result = self.convert_text(text, config)
            results.append(result)
        
        return results
    
    def get_conversion_statistics(self) -> Dict[str, Any]:
        """è·å–è½¬æ¢ç»Ÿè®¡ä¿¡æ¯"""
        
        if not self.conversion_history:
            return {}
        
        total_conversions = len(self.conversion_history)
        avg_quality = sum(r.quality_score for r in self.conversion_history) / total_conversions
        avg_confidence = sum(r.confidence_score for r in self.conversion_history) / total_conversions
        
        vocab_changes_count = sum(len(r.vocabulary_changes) for r in self.conversion_history)
        
        return {
            "total_conversions": total_conversions,
            "average_quality_score": round(avg_quality, 3),
            "average_confidence_score": round(avg_confidence, 3),
            "total_vocabulary_changes": vocab_changes_count,
            "average_changes_per_conversion": round(vocab_changes_count / total_conversions, 2)
        }
    
    def save_conversion_history(self, file_path: str):
        """ä¿å­˜è½¬æ¢å†å²"""
        
        try:
            history_data = [asdict(result) for result in self.conversion_history]
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"è½¬æ¢å†å²å·²ä¿å­˜åˆ°: {file_path}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è½¬æ¢å†å²å¤±è´¥: {e}")
    
    def generate_conversion_report(self, file_path: str):
        """ç”Ÿæˆè½¬æ¢æŠ¥å‘Š"""
        
        stats = self.get_conversion_statistics()
        
        report_content = f"""# æ™ºèƒ½æ–‡é£è½¬æ¢å™¨ - è½¬æ¢æŠ¥å‘Š

## ğŸ“Š è½¬æ¢ç»Ÿè®¡

- **æ€»è½¬æ¢æ¬¡æ•°**: {stats.get('total_conversions', 0)}
- **å¹³å‡è´¨é‡è¯„åˆ†**: {stats.get('average_quality_score', 0.0):.3f}
- **å¹³å‡ç½®ä¿¡åº¦**: {stats.get('average_confidence_score', 0.0):.3f}
- **æ€»è¯æ±‡æ›¿æ¢æ•°**: {stats.get('total_vocabulary_changes', 0)}
- **å¹³å‡æ¯æ¬¡æ›¿æ¢æ•°**: {stats.get('average_changes_per_conversion', 0.0):.2f}

## ğŸ“ è½¬æ¢ç¤ºä¾‹

"""
        
        # æ·»åŠ æœ€è¿‘å‡ ä¸ªè½¬æ¢ç¤ºä¾‹
        for i, result in enumerate(self.conversion_history[-3:]):
            report_content += f"""
### ç¤ºä¾‹ {i+1}

**åŸæ–‡**: {result.original_text[:100]}{'...' if len(result.original_text) > 100 else ''}

**è½¬æ¢å**: {result.converted_text[:100]}{'...' if len(result.converted_text) > 100 else ''}

**è´¨é‡è¯„åˆ†**: {result.quality_score:.3f}
**ç½®ä¿¡åº¦**: {result.confidence_score:.3f}
**è¯æ±‡æ›¿æ¢**: {len(result.vocabulary_changes)}ä¸ª

"""
        
        report_content += f"""
## ğŸ”§ è½¬æ¢é…ç½®å»ºè®®

æ ¹æ®å½“å‰è½¬æ¢æ•ˆæœï¼Œå»ºè®®çš„æœ€ä¼˜é…ç½®ï¼š
- **è¯æ±‡è½¬æ¢å¼ºåº¦**: {'high' if stats.get('average_quality_score', 0) > 0.8 else 'medium'}
- **å¥å¼é‡æ„**: å»ºè®®å¯ç”¨
- **ä¿®è¾å¢å¼º**: å»ºè®®å¯ç”¨
- **è¯­ä¹‰ä¿æŒ**: å»ºè®®å¯ç”¨

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {str(self.logger.handlers[0].formatter.formatTime(self.logger.handlers[0], None) if self.logger.handlers else 'Unknown')}*
"""
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            self.logger.info(f"è½¬æ¢æŠ¥å‘Šå·²ç”Ÿæˆ: {file_path}")
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè½¬æ¢æŠ¥å‘Šå¤±è´¥: {e}")


def create_intelligent_converter(analyzer: Optional[ClassicalStyleAnalyzer] = None,
                               template_library: Optional[StyleTemplateLibrary] = None) -> IntelligentStyleConverter:
    """åˆ›å»ºæ™ºèƒ½æ–‡é£è½¬æ¢å™¨å®ä¾‹"""
    return IntelligentStyleConverter(analyzer, template_library) 