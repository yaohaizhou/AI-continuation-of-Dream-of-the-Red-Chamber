"""
é£æ ¼ç›¸ä¼¼åº¦è¯„ä¼°å™¨

é‡åŒ–è¯„ä¼°æ–‡æœ¬ä¸çº¢æ¥¼æ¢¦åŸè‘—çš„é£æ ¼ç›¸ä¼¼åº¦ï¼Œä¸ºæ™ºèƒ½æ–‡é£è½¬æ¢å™¨æä¾›
è´¨é‡åé¦ˆå’Œä¼˜åŒ–å»ºè®®ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- å¤šç»´åº¦ç›¸ä¼¼åº¦è®¡ç®—ï¼šè¯æ±‡ã€å¥å¼ã€ä¿®è¾ã€ç§°è°“ã€æ•´ä½“éŸµå‘³
- è½¬æ¢æ•ˆæœé‡åŒ–è¯„ä¼°ï¼šä¸åŸè‘—çš„åŒ¹é…ç¨‹åº¦è¯„åˆ†
- å®æ—¶è´¨é‡ç›‘æ§ï¼šè½¬æ¢å‰åå¯¹æ¯”åˆ†æ
- æ‰¹é‡è¯„ä¼°ç»Ÿè®¡ï¼šå¤šæ–‡æœ¬ç›¸ä¼¼åº¦æ‰¹é‡è®¡ç®—å’ŒæŠ¥å‘Š
"""

import re
import json
import jieba
import logging
import numpy as np
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from collections import Counter, defaultdict
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

from .classical_style_analyzer import ClassicalStyleAnalyzer, StyleFeatures
from .intelligent_style_converter import ConversionResult

@dataclass
class SimilarityScores:
    """ç›¸ä¼¼åº¦è¯„åˆ†"""
    vocabulary_similarity: float        # è¯æ±‡ç›¸ä¼¼åº¦ (0-1)
    sentence_similarity: float          # å¥å¼ç›¸ä¼¼åº¦ (0-1)  
    rhetorical_similarity: float        # ä¿®è¾ç›¸ä¼¼åº¦ (0-1)
    addressing_similarity: float        # ç§°è°“ç›¸ä¼¼åº¦ (0-1)
    overall_style_similarity: float     # æ•´ä½“é£æ ¼ç›¸ä¼¼åº¦ (0-1)
    total_score: float                  # ç»¼åˆè¯„åˆ† (0-100)
    grade: str                          # è¯„åˆ†ç­‰çº§ (A+/A/B+/B/C/D)

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœ"""
    original_text: str                  # åŸå§‹æ–‡æœ¬
    evaluated_text: str                 # è¢«è¯„ä¼°æ–‡æœ¬
    similarity_scores: SimilarityScores # ç›¸ä¼¼åº¦è¯„åˆ†
    detailed_analysis: Dict[str, Any]   # è¯¦ç»†åˆ†æç»“æœ
    improvement_suggestions: List[str]   # æ”¹è¿›å»ºè®®
    evaluation_time: float              # è¯„ä¼°è€—æ—¶
    baseline_comparison: Dict[str, float] # ä¸åŸºå‡†çš„å¯¹æ¯”

@dataclass
class BatchEvaluationResult:
    """æ‰¹é‡è¯„ä¼°ç»“æœ"""
    total_evaluations: int              # æ€»è¯„ä¼°æ•°
    average_scores: SimilarityScores    # å¹³å‡è¯„åˆ†
    score_distribution: Dict[str, int]  # è¯„åˆ†åˆ†å¸ƒ
    best_results: List[EvaluationResult] # æœ€ä½³ç»“æœ
    worst_results: List[EvaluationResult] # æœ€å·®ç»“æœ
    evaluation_statistics: Dict[str, Any] # è¯„ä¼°ç»Ÿè®¡

class StyleSimilarityEvaluator:
    """é£æ ¼ç›¸ä¼¼åº¦è¯„ä¼°å™¨"""
    
    def __init__(self, 
                 analyzer: Optional[ClassicalStyleAnalyzer] = None,
                 original_text_path: str = "data/raw/hongloumeng_80.md"):
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self.analyzer = analyzer or ClassicalStyleAnalyzer(original_text_path)
        self.original_text_path = Path(original_text_path)
        
        # åˆå§‹åŒ–åŸè‘—åŸºå‡†
        self._init_baseline_model()
        
        # è¯„ä¼°æƒé‡é…ç½®
        self.similarity_weights = {
            'vocabulary': 0.30,      # è¯æ±‡åŒ¹é…åº¦æƒé‡
            'sentence': 0.25,        # å¥å¼ç›¸ä¼¼åº¦æƒé‡  
            'rhetorical': 0.25,      # ä¿®è¾ä¸°å¯Œåº¦æƒé‡
            'addressing': 0.10,      # ç§°è°“å‡†ç¡®åº¦æƒé‡
            'overall_style': 0.10    # æ•´ä½“éŸµå‘³æƒé‡
        }
        
        # è¯„åˆ†ç­‰çº§æ ‡å‡†
        self.grade_thresholds = {
            'A+': 90, 'A': 80, 'B+': 70, 'B': 60, 'C': 50, 'D': 0
        }
        
        # è¯„ä¼°å†å²
        self.evaluation_history: List[EvaluationResult] = []
        
    def _init_baseline_model(self):
        """åˆå§‹åŒ–åŸè‘—åŸºå‡†æ¨¡å‹"""
        
        self.logger.info("åˆå§‹åŒ–çº¢æ¥¼æ¢¦åŸè‘—åŸºå‡†æ¨¡å‹...")
        
        try:
            # åŠ è½½åŸè‘—æ–‡æœ¬
            if self.original_text_path.exists():
                with open(self.original_text_path, 'r', encoding='utf-8') as f:
                    self.original_text = f.read()
                
                # åˆ†æåŸè‘—æ–‡é£ç‰¹å¾
                self.baseline_features = self.analyzer.analyze_text(self.original_text)
                
                # æ„å»ºè¯æ±‡ç‰¹å¾åº“
                self._build_vocabulary_baseline()
                
                # æ„å»ºå¥å¼ç‰¹å¾åº“
                self._build_sentence_baseline()
                
                # æ„å»ºä¿®è¾ç‰¹å¾åº“
                self._build_rhetorical_baseline()
                
                # æ„å»ºTF-IDFå‘é‡åŒ–å™¨
                self._build_tfidf_baseline()
                
                self.logger.info("åŸè‘—åŸºå‡†æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
                
            else:
                self.logger.warning(f"åŸè‘—æ–‡ä»¶æœªæ‰¾åˆ°: {self.original_text_path}")
                self.original_text = ""
                self.baseline_features = None
                
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–åŸºå‡†æ¨¡å‹å¤±è´¥: {e}")
            self.original_text = ""
            self.baseline_features = None
    
    def _build_vocabulary_baseline(self):
        """æ„å»ºè¯æ±‡åŸºå‡†"""
        
        # åŸè‘—è¯æ±‡åˆ†å¸ƒ
        words = list(jieba.cut(self.original_text))
        self.baseline_vocab_freq = Counter(words)
        self.baseline_vocab_set = set(words)
        
        # å¤å…¸è¯æ±‡ç‰¹å¾
        self.baseline_classical_words = set()
        for category, word_list in self.analyzer.classical_words.items():
            self.baseline_classical_words.update(word_list)
        
        # è¯æ±‡ä¸°å¯Œåº¦
        self.baseline_vocab_diversity = len(set(words)) / len(words) if words else 0
        
    def _build_sentence_baseline(self):
        """æ„å»ºå¥å¼åŸºå‡†"""
        
        # å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›â€¦]+', self.original_text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # å¥é•¿åˆ†å¸ƒ
        sentence_lengths = [len(s) for s in sentences]
        self.baseline_avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0
        self.baseline_sentence_length_std = np.std(sentence_lengths) if sentence_lengths else 0
        
        # å¤å…¸å¥å¼æ¨¡å¼
        self.baseline_classical_patterns = {
            'åªè§': len(re.findall(r'åªè§', self.original_text)),
            'å´è¯´': len(re.findall(r'å´è¯´', self.original_text)),
            'ä½†è§': len(re.findall(r'ä½†è§', self.original_text)),
            'åŸæ¥': len(re.findall(r'åŸæ¥', self.original_text)),
        }
        
        # è¯­æ°”è¯ä½¿ç”¨
        self.baseline_modal_particles = {
            'ä¹Ÿ': len(re.findall(r'ä¹Ÿ', self.original_text)),
            'è€…': len(re.findall(r'è€…', self.original_text)),
            'çŸ£': len(re.findall(r'çŸ£', self.original_text)),
            'å“‰': len(re.findall(r'å“‰', self.original_text)),
        }
        
    def _build_rhetorical_baseline(self):
        """æ„å»ºä¿®è¾åŸºå‡†"""
        
        # æ¯”å–»æ¨¡å¼
        metaphor_patterns = [
            r'å¦‚.*èˆ¬', r'ä¼¼.*æ ·', r'åƒ.*ä¸€æ ·', r'å¥½æ¯”.*',
            r'å¦‚.*ä¸€èˆ¬', r'ä»¿ä½›.*', r'å®›å¦‚.*', r'çŠ¹å¦‚.*'
        ]
        self.baseline_metaphor_count = sum(
            len(re.findall(pattern, self.original_text)) for pattern in metaphor_patterns
        )
        
        # å¯¹å¶æ¨¡å¼  
        parallelism_patterns = [
            r'(\w+)å¯¹(\w+)', r'(\w+)é…(\w+)', r'(\w+)ï¼Œ(\w+)'
        ]
        self.baseline_parallelism_count = sum(
            len(re.findall(pattern, self.original_text)) for pattern in parallelism_patterns
        )
        
        # æ’æ¯”æ¨¡å¼
        enumeration_patterns = [
            r'(\w+)ï¼Œ(\w+)ï¼Œ(\w+)', r'(\w+)ä¹Ÿ(\w+)ä¹Ÿ(\w+)ä¹Ÿ'
        ]
        self.baseline_enumeration_count = sum(
            len(re.findall(pattern, self.original_text)) for pattern in enumeration_patterns
        )
        
        # ä¿®è¾å¯†åº¦
        text_length = len(self.original_text)
        self.baseline_rhetorical_density = (
            self.baseline_metaphor_count + 
            self.baseline_parallelism_count + 
            self.baseline_enumeration_count
        ) / text_length if text_length > 0 else 0
        
    def _build_tfidf_baseline(self):
        """æ„å»ºTF-IDFåŸºå‡†"""
        
        try:
            # åˆ†ç« èŠ‚å¤„ç†åŸè‘—æ–‡æœ¬
            chapter_pattern = r'ç¬¬\w+å›'
            chapters = re.split(chapter_pattern, self.original_text)
            chapters = [ch.strip() for ch in chapters if ch.strip() and len(ch) > 100]
            
            if len(chapters) < 5:
                # å¦‚æœç« èŠ‚åˆ†å‰²å¤±è´¥ï¼Œä½¿ç”¨æ®µè½åˆ†å‰²
                chapters = self.original_text.split('\n\n')
                chapters = [ch.strip() for ch in chapters if ch.strip() and len(ch) > 100]
            
            # æ„å»ºTF-IDFå‘é‡åŒ–å™¨
            self.tfidf_vectorizer = TfidfVectorizer(
                tokenizer=lambda x: list(jieba.cut(x)),
                lowercase=False,
                max_features=5000,
                min_df=2,
                max_df=0.95
            )
            
            if chapters:
                self.baseline_tfidf_matrix = self.tfidf_vectorizer.fit_transform(chapters)
                self.baseline_tfidf_mean = np.mean(self.baseline_tfidf_matrix.toarray(), axis=0)
            else:
                self.baseline_tfidf_matrix = None
                self.baseline_tfidf_mean = None
                
        except Exception as e:
            self.logger.warning(f"TF-IDFåŸºå‡†æ„å»ºå¤±è´¥: {e}")
            self.tfidf_vectorizer = None
            self.baseline_tfidf_matrix = None
            self.baseline_tfidf_mean = None
    
    def evaluate_similarity(self, 
                           text: str, 
                           original_text: Optional[str] = None,
                           detailed: bool = True) -> EvaluationResult:
        """è¯„ä¼°æ–‡æœ¬ä¸åŸè‘—çš„é£æ ¼ç›¸ä¼¼åº¦"""
        
        start_time = time.time()
        
        self.logger.info(f"å¼€å§‹è¯„ä¼°æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œé•¿åº¦: {len(text)}")
        
        # åˆ†æå¾…è¯„ä¼°æ–‡æœ¬çš„é£æ ¼ç‰¹å¾
        text_features = self.analyzer.analyze_text(text)
        
        # è®¡ç®—å„ç»´åº¦ç›¸ä¼¼åº¦
        vocabulary_sim = self._calculate_vocabulary_similarity(text, text_features)
        sentence_sim = self._calculate_sentence_similarity(text, text_features)
        rhetorical_sim = self._calculate_rhetorical_similarity(text, text_features)
        addressing_sim = self._calculate_addressing_similarity(text, text_features)
        overall_sim = self._calculate_overall_similarity(text)
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = (
            vocabulary_sim * self.similarity_weights['vocabulary'] +
            sentence_sim * self.similarity_weights['sentence'] +
            rhetorical_sim * self.similarity_weights['rhetorical'] +
            addressing_sim * self.similarity_weights['addressing'] +
            overall_sim * self.similarity_weights['overall_style']
        ) * 100
        
        # ç¡®å®šè¯„åˆ†ç­‰çº§
        grade = self._determine_grade(total_score)
        
        # åˆ›å»ºç›¸ä¼¼åº¦è¯„åˆ†å¯¹è±¡
        similarity_scores = SimilarityScores(
            vocabulary_similarity=vocabulary_sim,
            sentence_similarity=sentence_sim,
            rhetorical_similarity=rhetorical_sim,
            addressing_similarity=addressing_sim,
            overall_style_similarity=overall_sim,
            total_score=total_score,
            grade=grade
        )
        
        # è¯¦ç»†åˆ†æ
        detailed_analysis = {}
        if detailed:
            detailed_analysis = self._generate_detailed_analysis(text, text_features, similarity_scores)
        
        # æ”¹è¿›å»ºè®®
        improvement_suggestions = self._generate_improvement_suggestions(similarity_scores, text_features)
        
        # ä¸åŸºå‡†çš„å¯¹æ¯”
        baseline_comparison = self._compare_with_baseline(text_features)
        
        # è®¡ç®—è¯„ä¼°è€—æ—¶
        evaluation_time = time.time() - start_time
        
        # åˆ›å»ºè¯„ä¼°ç»“æœ
        result = EvaluationResult(
            original_text=original_text or "",
            evaluated_text=text,
            similarity_scores=similarity_scores,
            detailed_analysis=detailed_analysis,
            improvement_suggestions=improvement_suggestions,
            evaluation_time=evaluation_time,
            baseline_comparison=baseline_comparison
        )
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        self.evaluation_history.append(result)
        
        self.logger.info(f"ç›¸ä¼¼åº¦è¯„ä¼°å®Œæˆï¼Œæ€»åˆ†: {total_score:.1f}, ç­‰çº§: {grade}")
        
        return result
    
    def _calculate_vocabulary_similarity(self, text: str, features: StyleFeatures) -> float:
        """è®¡ç®—è¯æ±‡ç›¸ä¼¼åº¦"""
        
        if not self.baseline_features:
            return 0.5  # é»˜è®¤å€¼
        
        # 1. å¤å…¸è¯æ±‡æ¯”ä¾‹ç›¸ä¼¼åº¦ (40%)
        text_classical_ratio = features.vocabulary.classical_word_ratio
        baseline_classical_ratio = self.baseline_features.vocabulary.classical_word_ratio
        classical_sim = 1 - abs(text_classical_ratio - baseline_classical_ratio)
        
        # 2. è¯æ±‡é‡å åº¦ (30%)
        text_words = set(jieba.cut(text))
        overlap_ratio = len(text_words & self.baseline_vocab_set) / len(text_words | self.baseline_vocab_set)
        
        # 3. è¯æ±‡ä¸°å¯Œåº¦ç›¸ä¼¼åº¦ (20%)
        text_diversity = len(set(jieba.cut(text))) / len(list(jieba.cut(text)))
        diversity_sim = 1 - abs(text_diversity - self.baseline_vocab_diversity)
        
        # 4. é«˜é¢‘å¤å…¸è¯æ±‡ä½¿ç”¨ (10%)
        text_classical_words = set()
        for word in jieba.cut(text):
            if word in self.baseline_classical_words:
                text_classical_words.add(word)
        
        classical_usage_ratio = len(text_classical_words) / len(self.baseline_classical_words) if self.baseline_classical_words else 0
        classical_usage_sim = min(1.0, classical_usage_ratio)
        
        # åŠ æƒå¹³å‡
        vocabulary_similarity = (
            classical_sim * 0.4 +
            overlap_ratio * 0.3 +
            diversity_sim * 0.2 +
            classical_usage_sim * 0.1
        )
        
        return max(0.0, min(1.0, vocabulary_similarity))
    
    def _calculate_sentence_similarity(self, text: str, features: StyleFeatures) -> float:
        """è®¡ç®—å¥å¼ç›¸ä¼¼åº¦"""
        
        if not self.baseline_features:
            return 0.5
        
        # 1. å¥é•¿åˆ†å¸ƒç›¸ä¼¼åº¦ (40%)
        text_avg_length = features.sentence.avg_sentence_length
        length_sim = 1 - abs(text_avg_length - self.baseline_avg_sentence_length) / max(text_avg_length, self.baseline_avg_sentence_length, 1)
        
        # 2. å¤å…¸å¥å¼ä½¿ç”¨é¢‘ç‡ (35%)
        text_classical_patterns = {
            'åªè§': len(re.findall(r'åªè§', text)),
            'å´è¯´': len(re.findall(r'å´è¯´', text)),
            'ä½†è§': len(re.findall(r'ä½†è§', text)),
            'åŸæ¥': len(re.findall(r'åŸæ¥', text)),
        }
        
        text_length = len(text)
        pattern_similarities = []
        for pattern, baseline_count in self.baseline_classical_patterns.items():
            text_count = text_classical_patterns.get(pattern, 0)
            baseline_freq = baseline_count / len(self.original_text) if self.original_text else 0
            text_freq = text_count / text_length if text_length > 0 else 0
            pattern_sim = 1 - abs(text_freq - baseline_freq) / max(text_freq, baseline_freq, 0.0001)
            pattern_similarities.append(pattern_sim)
        
        pattern_sim = np.mean(pattern_similarities) if pattern_similarities else 0.5
        
        # 3. è¯­æ°”è¯ä½¿ç”¨ (15%)
        modal_similarities = []
        for particle, baseline_count in self.baseline_modal_particles.items():
            text_count = len(re.findall(particle, text))
            baseline_freq = baseline_count / len(self.original_text) if self.original_text else 0
            text_freq = text_count / text_length if text_length > 0 else 0
            modal_sim = 1 - abs(text_freq - baseline_freq) / max(text_freq, baseline_freq, 0.0001)
            modal_similarities.append(modal_sim)
        
        modal_sim = np.mean(modal_similarities) if modal_similarities else 0.5
        
        # 4. å¥å¼å¤æ‚åº¦ (10%)
        text_complexity = features.sentence.sentence_complexity
        baseline_complexity = self.baseline_features.sentence.sentence_complexity
        complexity_sim = 1 - abs(text_complexity - baseline_complexity) / max(text_complexity, baseline_complexity, 1)
        
        # åŠ æƒå¹³å‡
        sentence_similarity = (
            length_sim * 0.4 +
            pattern_sim * 0.35 +
            modal_sim * 0.15 +
            complexity_sim * 0.1
        )
        
        return max(0.0, min(1.0, sentence_similarity))
    
    def _calculate_rhetorical_similarity(self, text: str, features: StyleFeatures) -> float:
        """è®¡ç®—ä¿®è¾ç›¸ä¼¼åº¦"""
        
        text_length = len(text)
        if text_length == 0:
            return 0.0
        
        # 1. æ¯”å–»ä½¿ç”¨é¢‘ç‡ (40%)
        metaphor_patterns = [
            r'å¦‚.*èˆ¬', r'ä¼¼.*æ ·', r'åƒ.*ä¸€æ ·', r'å¥½æ¯”.*',
            r'å¦‚.*ä¸€èˆ¬', r'ä»¿ä½›.*', r'å®›å¦‚.*', r'çŠ¹å¦‚.*'
        ]
        text_metaphor_count = sum(len(re.findall(pattern, text)) for pattern in metaphor_patterns)
        text_metaphor_density = text_metaphor_count / text_length
        baseline_metaphor_density = self.baseline_metaphor_count / len(self.original_text) if self.original_text else 0
        metaphor_sim = 1 - abs(text_metaphor_density - baseline_metaphor_density) / max(text_metaphor_density, baseline_metaphor_density, 0.0001)
        
        # 2. å¯¹å¶ä½¿ç”¨é¢‘ç‡ (30%)
        parallelism_patterns = [r'(\w+)å¯¹(\w+)', r'(\w+)é…(\w+)']
        text_parallelism_count = sum(len(re.findall(pattern, text)) for pattern in parallelism_patterns)
        text_parallelism_density = text_parallelism_count / text_length
        baseline_parallelism_density = self.baseline_parallelism_count / len(self.original_text) if self.original_text else 0
        parallelism_sim = 1 - abs(text_parallelism_density - baseline_parallelism_density) / max(text_parallelism_density, baseline_parallelism_density, 0.0001)
        
        # 3. æ’æ¯”ä½¿ç”¨é¢‘ç‡ (20%)
        enumeration_patterns = [r'(\w+)ï¼Œ(\w+)ï¼Œ(\w+)', r'(\w+)ä¹Ÿ(\w+)ä¹Ÿ(\w+)ä¹Ÿ']
        text_enumeration_count = sum(len(re.findall(pattern, text)) for pattern in enumeration_patterns)
        text_enumeration_density = text_enumeration_count / text_length
        baseline_enumeration_density = self.baseline_enumeration_count / len(self.original_text) if self.original_text else 0
        enumeration_sim = 1 - abs(text_enumeration_density - baseline_enumeration_density) / max(text_enumeration_density, baseline_enumeration_density, 0.0001)
        
        # 4. æ•´ä½“ä¿®è¾å¯†åº¦ (10%)
        text_rhetorical_density = features.rhetorical.rhetorical_density
        rhetorical_density_sim = 1 - abs(text_rhetorical_density - self.baseline_rhetorical_density) / max(text_rhetorical_density, self.baseline_rhetorical_density, 0.0001)
        
        # åŠ æƒå¹³å‡
        rhetorical_similarity = (
            metaphor_sim * 0.4 +
            parallelism_sim * 0.3 +
            enumeration_sim * 0.2 +
            rhetorical_density_sim * 0.1
        )
        
        return max(0.0, min(1.0, rhetorical_similarity))
    
    def _calculate_addressing_similarity(self, text: str, features: StyleFeatures) -> float:
        """è®¡ç®—ç§°è°“ç›¸ä¼¼åº¦"""
        
        if not self.baseline_features:
            return 0.5
        
        # 1. èº«ä»½ä¸€è‡´æ€§ (60%)
        text_identity_consistency = features.addressing.identity_consistency
        baseline_identity_consistency = self.baseline_features.addressing.identity_consistency
        identity_sim = 1 - abs(text_identity_consistency - baseline_identity_consistency)
        
        # 2. æƒ…å¢ƒé€‚åº”æ€§ (40%)
        text_contextual_appropriateness = features.addressing.contextual_appropriateness
        baseline_contextual_appropriateness = self.baseline_features.addressing.contextual_appropriateness
        contextual_sim = 1 - abs(text_contextual_appropriateness - baseline_contextual_appropriateness)
        
        # åŠ æƒå¹³å‡
        addressing_similarity = (
            identity_sim * 0.6 +
            contextual_sim * 0.4
        )
        
        return max(0.0, min(1.0, addressing_similarity))
    
    def _calculate_overall_similarity(self, text: str) -> float:
        """è®¡ç®—æ•´ä½“é£æ ¼ç›¸ä¼¼åº¦"""
        
        if not self.tfidf_vectorizer or self.baseline_tfidf_mean is None:
            return 0.5
        
        try:
            # ä½¿ç”¨TF-IDFå‘é‡è®¡ç®—æ•´ä½“ç›¸ä¼¼åº¦
            text_tfidf = self.tfidf_vectorizer.transform([text])
            text_vector = text_tfidf.toarray()[0]
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = cosine_similarity([text_vector], [self.baseline_tfidf_mean])[0][0]
            
            # è°ƒæ•´åˆ°0-1èŒƒå›´
            return max(0.0, min(1.0, (similarity + 1) / 2))
            
        except Exception as e:
            self.logger.warning(f"è®¡ç®—æ•´ä½“ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.5
    
    def _determine_grade(self, score: float) -> str:
        """ç¡®å®šè¯„åˆ†ç­‰çº§"""
        for grade, threshold in self.grade_thresholds.items():
            if score >= threshold:
                return grade
        return 'D'
    
    def _generate_detailed_analysis(self, text: str, features: StyleFeatures, scores: SimilarityScores) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†åˆ†æ"""
        
        analysis = {
            'text_statistics': {
                'total_characters': len(text),
                'total_words': len(list(jieba.cut(text))),
                'unique_words': len(set(jieba.cut(text))),
                'avg_sentence_length': features.sentence.avg_sentence_length,
                'sentence_complexity': features.sentence.sentence_complexity
            },
            'vocabulary_analysis': {
                'classical_word_ratio': features.vocabulary.classical_word_ratio,
                'modern_words_detected': len(features.vocabulary.modern_words_detected),
                'emotional_color_distribution': len(features.vocabulary.emotional_color_words)
            },
            'rhetorical_analysis': {
                'metaphor_count': features.rhetorical.metaphor_simile_count,
                'parallelism_count': features.rhetorical.parallelism_count,
                'allusion_count': features.rhetorical.allusion_count,
                'rhetorical_density': features.rhetorical.rhetorical_density
            },
            'style_comparison': {
                'vs_baseline_classical_ratio': f"{features.vocabulary.classical_word_ratio:.3f} vs {self.baseline_features.vocabulary.classical_word_ratio:.3f}" if self.baseline_features else "N/A",
                'vs_baseline_sentence_length': f"{features.sentence.avg_sentence_length:.1f} vs {self.baseline_avg_sentence_length:.1f}",
                'vs_baseline_rhetorical_density': f"{features.rhetorical.rhetorical_density:.4f} vs {self.baseline_rhetorical_density:.4f}"
            }
        }
        
        return analysis
    
    def _generate_improvement_suggestions(self, scores: SimilarityScores, features: StyleFeatures) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        suggestions = []
        
        # è¯æ±‡ç›¸ä¼¼åº¦å»ºè®®
        if scores.vocabulary_similarity < 0.7:
            suggestions.append("å»ºè®®å¢åŠ å¤å…¸è¯æ±‡çš„ä½¿ç”¨ï¼Œå‡å°‘ç°ä»£åŒ–è¡¨è¾¾")
            if features.vocabulary.classical_word_ratio < 0.5:
                suggestions.append("å¤å…¸è¯æ±‡æ¯”ä¾‹åä½ï¼Œå»ºè®®å‚è€ƒæ–‡ä½“æ¨¡æ¿åº“å¢åŠ å¤å…¸ç”¨è¯")
        
        # å¥å¼ç›¸ä¼¼åº¦å»ºè®®
        if scores.sentence_similarity < 0.7:
            suggestions.append("å»ºè®®å¢åŠ å¤å…¸å¥å¼çš„ä½¿ç”¨ï¼Œå¦‚'åªè§'ã€'å´è¯´'ç­‰å¼€å¤´")
            if features.sentence.avg_sentence_length < 15:
                suggestions.append("å¥å­åçŸ­ï¼Œå»ºè®®é€‚å½“å¢åŠ å¥å­é•¿åº¦å’Œå¤æ‚åº¦")
        
        # ä¿®è¾ç›¸ä¼¼åº¦å»ºè®®
        if scores.rhetorical_similarity < 0.7:
            suggestions.append("å»ºè®®å¢åŠ æ¯”å–»ã€å¯¹å¶ç­‰ä¿®è¾æ‰‹æ³•çš„ä½¿ç”¨")
            if features.rhetorical.metaphor_simile_count == 0:
                suggestions.append("ç¼ºä¹æ¯”å–»ä¿®è¾ï¼Œå»ºè®®æ·»åŠ 'å¦‚...èˆ¬'ã€'ä¼¼...æ ·'ç­‰è¡¨è¾¾")
        
        # ç§°è°“ç›¸ä¼¼åº¦å»ºè®®
        if scores.addressing_similarity < 0.7:
            suggestions.append("å»ºè®®æ£€æŸ¥äººç‰©ç§°è°“çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§")
        
        # æ•´ä½“é£æ ¼å»ºè®®
        if scores.overall_style_similarity < 0.6:
            suggestions.append("æ•´ä½“é£æ ¼ä¸åŸè‘—å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®ç»¼åˆè¿ç”¨å¤å…¸æ–‡é£è½¬æ¢å™¨è¿›è¡Œä¼˜åŒ–")
        
        if not suggestions:
            suggestions.append("æ–‡é£ç›¸ä¼¼åº¦è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå¤å…¸æ–‡å­¦ç‰¹è‰²")
        
        return suggestions
    
    def _compare_with_baseline(self, features: StyleFeatures) -> Dict[str, float]:
        """ä¸åŸºå‡†è¿›è¡Œå¯¹æ¯”"""
        
        if not self.baseline_features:
            return {}
        
        comparison = {
            'classical_word_ratio_diff': features.vocabulary.classical_word_ratio - self.baseline_features.vocabulary.classical_word_ratio,
            'avg_sentence_length_diff': features.sentence.avg_sentence_length - self.baseline_avg_sentence_length,
            'rhetorical_density_diff': features.rhetorical.rhetorical_density - self.baseline_rhetorical_density,
            'literary_elegance_diff': features.literary_elegance - self.baseline_features.literary_elegance,
            'classical_authenticity_diff': features.classical_authenticity - self.baseline_features.classical_authenticity
        }
        
        return comparison
    
    def evaluate_conversion_result(self, conversion_result: ConversionResult) -> EvaluationResult:
        """è¯„ä¼°è½¬æ¢ç»“æœçš„ç›¸ä¼¼åº¦"""
        
        return self.evaluate_similarity(
            text=conversion_result.converted_text,
            original_text=conversion_result.original_text,
            detailed=True
        )
    
    def batch_evaluate(self, texts: List[str], detailed: bool = False) -> BatchEvaluationResult:
        """æ‰¹é‡è¯„ä¼°å¤šä¸ªæ–‡æœ¬"""
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡è¯„ä¼° {len(texts)} ä¸ªæ–‡æœ¬")
        
        results = []
        for i, text in enumerate(texts):
            try:
                result = self.evaluate_similarity(text, detailed=detailed)
                results.append(result)
                
                if (i + 1) % 10 == 0:
                    self.logger.info(f"å·²å®Œæˆ {i + 1}/{len(texts)} ä¸ªæ–‡æœ¬çš„è¯„ä¼°")
                    
            except Exception as e:
                self.logger.error(f"è¯„ä¼°ç¬¬ {i + 1} ä¸ªæ–‡æœ¬å¤±è´¥: {e}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if not results:
            return BatchEvaluationResult(
                total_evaluations=0,
                average_scores=SimilarityScores(0, 0, 0, 0, 0, 0, 'D'),
                score_distribution={},
                best_results=[],
                worst_results=[],
                evaluation_statistics={}
            )
        
        # å¹³å‡è¯„åˆ†
        avg_vocabulary = np.mean([r.similarity_scores.vocabulary_similarity for r in results])
        avg_sentence = np.mean([r.similarity_scores.sentence_similarity for r in results])
        avg_rhetorical = np.mean([r.similarity_scores.rhetorical_similarity for r in results])
        avg_addressing = np.mean([r.similarity_scores.addressing_similarity for r in results])
        avg_overall = np.mean([r.similarity_scores.overall_style_similarity for r in results])
        avg_total = np.mean([r.similarity_scores.total_score for r in results])
        avg_grade = self._determine_grade(avg_total)
        
        average_scores = SimilarityScores(
            vocabulary_similarity=avg_vocabulary,
            sentence_similarity=avg_sentence,
            rhetorical_similarity=avg_rhetorical,
            addressing_similarity=avg_addressing,
            overall_style_similarity=avg_overall,
            total_score=avg_total,
            grade=avg_grade
        )
        
        # è¯„åˆ†åˆ†å¸ƒ
        grade_counts = Counter(r.similarity_scores.grade for r in results)
        score_distribution = dict(grade_counts)
        
        # æœ€ä½³å’Œæœ€å·®ç»“æœ
        results_sorted = sorted(results, key=lambda x: x.similarity_scores.total_score, reverse=True)
        best_results = results_sorted[:3]
        worst_results = results_sorted[-3:]
        
        # è¯„ä¼°ç»Ÿè®¡
        evaluation_statistics = {
            'total_evaluation_time': sum(r.evaluation_time for r in results),
            'avg_evaluation_time': np.mean([r.evaluation_time for r in results]),
            'score_std': np.std([r.similarity_scores.total_score for r in results]),
            'score_range': (
                min(r.similarity_scores.total_score for r in results),
                max(r.similarity_scores.total_score for r in results)
            )
        }
        
        batch_result = BatchEvaluationResult(
            total_evaluations=len(results),
            average_scores=average_scores,
            score_distribution=score_distribution,
            best_results=best_results,
            worst_results=worst_results,
            evaluation_statistics=evaluation_statistics
        )
        
        self.logger.info(f"æ‰¹é‡è¯„ä¼°å®Œæˆï¼Œå¹³å‡è¯„åˆ†: {avg_total:.1f}, ç­‰çº§: {avg_grade}")
        
        return batch_result
    
    def get_evaluation_statistics(self) -> Dict[str, Any]:
        """è·å–è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯"""
        
        if not self.evaluation_history:
            return {}
        
        scores = [r.similarity_scores.total_score for r in self.evaluation_history]
        
        return {
            'total_evaluations': len(self.evaluation_history),
            'average_score': np.mean(scores),
            'score_std': np.std(scores),
            'score_range': (min(scores), max(scores)),
            'grade_distribution': dict(Counter(r.similarity_scores.grade for r in self.evaluation_history)),
            'avg_evaluation_time': np.mean([r.evaluation_time for r in self.evaluation_history])
        }
    
    def save_evaluation_history(self, file_path: str):
        """ä¿å­˜è¯„ä¼°å†å²"""
        
        try:
            history_data = [asdict(result) for result in self.evaluation_history]
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"è¯„ä¼°å†å²å·²ä¿å­˜åˆ°: {file_path}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¯„ä¼°å†å²å¤±è´¥: {e}")
    
    def generate_evaluation_report(self, file_path: str, batch_result: Optional[BatchEvaluationResult] = None):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        
        stats = self.get_evaluation_statistics()
        
        report_content = f"""# é£æ ¼ç›¸ä¼¼åº¦è¯„ä¼°å™¨ - è¯„ä¼°æŠ¥å‘Š

## ğŸ“Š è¯„ä¼°ç»Ÿè®¡

- **æ€»è¯„ä¼°æ¬¡æ•°**: {stats.get('total_evaluations', 0)}
- **å¹³å‡è¯„åˆ†**: {stats.get('average_score', 0.0):.1f}
- **è¯„åˆ†æ ‡å‡†å·®**: {stats.get('score_std', 0.0):.2f}
- **è¯„åˆ†èŒƒå›´**: {stats.get('score_range', (0, 0))}
- **å¹³å‡è¯„ä¼°æ—¶é—´**: {stats.get('avg_evaluation_time', 0.0):.3f}ç§’

## ğŸ“ˆ è¯„åˆ†åˆ†å¸ƒ

"""
        
        # æ·»åŠ è¯„åˆ†åˆ†å¸ƒ
        grade_dist = stats.get('grade_distribution', {})
        for grade, count in sorted(grade_dist.items()):
            percentage = count / stats.get('total_evaluations', 1) * 100
            report_content += f"- **{grade}çº§**: {count}æ¬¡ ({percentage:.1f}%)\n"
        
        # æ‰¹é‡è¯„ä¼°ç»“æœ
        if batch_result:
            report_content += f"""

## ğŸ¯ æ‰¹é‡è¯„ä¼°ç»“æœ

- **æ‰¹é‡è¯„ä¼°æ–‡æœ¬æ•°**: {batch_result.total_evaluations}
- **å¹³å‡ç»¼åˆè¯„åˆ†**: {batch_result.average_scores.total_score:.1f}
- **å¹³å‡ç­‰çº§**: {batch_result.average_scores.grade}

### ğŸ“Š è¯¦ç»†ç»´åº¦è¯„åˆ†

- **è¯æ±‡ç›¸ä¼¼åº¦**: {batch_result.average_scores.vocabulary_similarity:.3f}
- **å¥å¼ç›¸ä¼¼åº¦**: {batch_result.average_scores.sentence_similarity:.3f}  
- **ä¿®è¾ç›¸ä¼¼åº¦**: {batch_result.average_scores.rhetorical_similarity:.3f}
- **ç§°è°“ç›¸ä¼¼åº¦**: {batch_result.average_scores.addressing_similarity:.3f}
- **æ•´ä½“é£æ ¼ç›¸ä¼¼åº¦**: {batch_result.average_scores.overall_style_similarity:.3f}

"""
        
        # æ·»åŠ æœ€è¿‘è¯„ä¼°ç¤ºä¾‹
        if self.evaluation_history:
            report_content += f"""

## ğŸ“ æœ€è¿‘è¯„ä¼°ç¤ºä¾‹

"""
            for i, result in enumerate(self.evaluation_history[-3:]):
                report_content += f"""
### ç¤ºä¾‹ {i+1}

**è¯„ä¼°æ–‡æœ¬**: {result.evaluated_text[:100]}{'...' if len(result.evaluated_text) > 100 else ''}

**ç»¼åˆè¯„åˆ†**: {result.similarity_scores.total_score:.1f} ({result.similarity_scores.grade}çº§)

**ç»´åº¦è¯„åˆ†**:
- è¯æ±‡ç›¸ä¼¼åº¦: {result.similarity_scores.vocabulary_similarity:.3f}
- å¥å¼ç›¸ä¼¼åº¦: {result.similarity_scores.sentence_similarity:.3f}
- ä¿®è¾ç›¸ä¼¼åº¦: {result.similarity_scores.rhetorical_similarity:.3f}
- ç§°è°“ç›¸ä¼¼åº¦: {result.similarity_scores.addressing_similarity:.3f}
- æ•´ä½“é£æ ¼: {result.similarity_scores.overall_style_similarity:.3f}

**æ”¹è¿›å»ºè®®**: {'; '.join(result.improvement_suggestions[:2])}

"""
        
        report_content += f"""

## ğŸ”§ è¯„ä¼°é…ç½®

### æƒé‡è®¾ç½®
- è¯æ±‡åŒ¹é…åº¦: {self.similarity_weights['vocabulary']:.0%}
- å¥å¼ç›¸ä¼¼åº¦: {self.similarity_weights['sentence']:.0%}
- ä¿®è¾ä¸°å¯Œåº¦: {self.similarity_weights['rhetorical']:.0%}
- ç§°è°“å‡†ç¡®åº¦: {self.similarity_weights['addressing']:.0%}
- æ•´ä½“éŸµå‘³: {self.similarity_weights['overall_style']:.0%}

### è¯„åˆ†ç­‰çº§
- A+: 90åˆ†ä»¥ä¸Š (å®Œå…¨ç¬¦åˆåŸè‘—é£æ ¼)
- A: 80-89åˆ† (åŸºæœ¬ç¬¦åˆï¼Œè½»å¾®å·®å¼‚)
- B+: 70-79åˆ† (éƒ¨åˆ†ç¬¦åˆï¼Œéœ€è¦ä¼˜åŒ–)
- B: 60-69åˆ† (é£æ ¼å·®å¼‚æ˜æ˜¾)
- C: 50-59åˆ† (é£æ ¼å·®å¼‚è¾ƒå¤§)
- D: 50åˆ†ä»¥ä¸‹ (å®Œå…¨ä¸ç¬¦åˆåŸè‘—é£æ ¼)

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        try:
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            self.logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {file_path}")
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šå¤±è´¥: {e}")


# å¯¼å…¥timeæ¨¡å—
import time

def create_style_similarity_evaluator(analyzer: Optional[ClassicalStyleAnalyzer] = None) -> StyleSimilarityEvaluator:
    """åˆ›å»ºé£æ ¼ç›¸ä¼¼åº¦è¯„ä¼°å™¨å®ä¾‹"""
    return StyleSimilarityEvaluator(analyzer) 